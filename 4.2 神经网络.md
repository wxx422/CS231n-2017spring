# 4.2 神经网络 Neural Network 

人们将神经网络与大脑和生物学上的其他种类做了大量类比，我们将会谈到一点这方面的内容，但是首先我们将它看做与大脑无关的一组函数。之前我们已经使用了很多这种线性计分函数，$f = Wx​$，我们将它作为需要优化的函数来使用。我们不用单变换的，如果我们依旧想在神经网络里用最简单形式，只需要将两个变换结合在一起。顶层有一个线性变换，然后和另一个构成两层神经网络。这个和第一个变换很相似，$W_1​$ 乘以 $x​$ 的一个矩阵乘法，然后我们得到这个中间变量，这个 0 和 $W​$ 取最大值的非线性函数，将线性层的最大值作为输出，这些非线性计算非常重要，我们以后会讲到。如果你只是在顶层将线性层合在一起，它们会被简化为一个线性函数。所以我们首先有线性层然后有这个非线性计算，继而在顶层再加入另一个线性层。最后我们得到一个计分函数，输出计分向量。一般来说，神经网络就是由简单函数构成的一组函数，在顶层堆叠在一起，我们用一种层次化的方式将他们堆叠起来，为了形成一个更复杂的非线性函数，所以这就是基本的多阶段分层计算。这可以说是我们实现它的主要手段，就像是矩阵乘法，这个线性层，我们只是将多个线性层堆在顶层，和其他非线性函数结合在一起。

People draw a lot of analogies between neural networks and the brain, and different types of biological inspirations, and we'll get to that in a little bit, but first let's talk about it, just looking at it as a function, as a class of functions without all of the brain stuff. So far we've worked a lot with this linear score function, $f = Wx$, and so we've been using this as a running example of a function that we want to optimize. Instead of using the single in your transformation, if we want a neural network where we can just, as the simplest form, just stack two of these together. Just a linear transformation on top of another one, in order to get a two-layer neural network. And what this looks like is first we have a matrix multiply of $W_1$ with $x$, and then we get this intermediate variable and we have this non-linear function of a max of zero with $W$, max with this output of this linear layer, and it's really important to have these non-linearities in place, which we'll talk about more later, because otherwise if you just stack linear layers on top of each other, they're just going to collapse to, like a single linear function. So we have our first linear layer and then we have this non-linearity, and then on top of this we'll add another linear layer. And then from here, finally we can get our score function, our output vector of scores. More broadly speaking, neural networks are a class of functions where we have simpler functions, that are stacked on top of each other, and we stack them in a hierarchical way in order to make up a more complex non-linear function, and so this is the idea of having, basically multiple stages of hierarchical computation. And so this is kind of the main way that we do this is by taking something like this matrix multiply, this linear layer, and we just stack multiple of these on top of each other with non-linear functions in-between.

-------------------------

<center>
    <image src = "./images/4.2_01_neural-networks.png"></iamge>
</center>
---------------------------------------------

一个可以帮我们解决这个问题的方法是如果我们还记得之前讲到过的线性计分函数，我们之前讨论过权重矩阵 $W$ 的每一行相当于一个模板。它是一个分类表达的模板，使我们正在寻找的具体分类输入，例如车的模板看起来像是略模糊的红色车，我们要在输入中找到它，计算它在车这一分类中的得分。有一个问题是这里只有一个范本。这里是这辆红色的车，而实际中我们有多种样式。我们可能要找的是一辆红色的车，也可能是一辆黄色的车，还有各式各样的车，所以这种多层网络，每个中间变量 $h$，$W_1$ 还是各式范本的集合，在 $h$ 中对这些范本都有得分，上面的另一层再将这些得分组合起来。因此可以说实际上我们的车分类应该有联系，既有我们正在寻找的红色的车，也有黄色的车。因为矩阵 $W_2$，是 $h$ 里所有向量的权重。

And so one thing that this can help solve is if we remember back to this linear score function that we were talking about, remember we discussed earlier how each row of our weight matrix $W​$ was something like a template. It was a template that sort of expressed, what we're looking for in the input for a specific class, so for example, the car template looks something like this kind of fuzzy red car, and we were looking for this in the input to compute the score for the car class. And we talked about one of the problems with this is there is only one template. There's this red car, whereas in practice, we actually have multiple modes, we might want a red car, or a yellow car, like all of these are different kinds of cats, and so what this kind of multiple layer network lets you do is, each of this intermediate variable $h​$, $W_1​$ can still be these kinds of templates, but now you have all of these scores for these templates in $h​$, and we can have another layer on top that's combining these together. So we can say that actually my car class should be connected to, we're looking for both red cars as well as yellow cars. Because we have this matrix $W_2​$ which is now a weighting of all of our vector in $h​$.

$W_1$ 和 输入 $x$ 直接关联，可用于说明，因为可以得到所有这些范本，$h$ 是对范本符合程度的得分，比方说红色车的得分是 2，黄色车或者别的车得分是 1。$W_1$ 里既包括了左侧脸的马，也包括右侧脸的马。所以 $W_1$ 是可以有很多种不同的范本，然后 $W_2$ 是所有范本的加权和。让你可以在多个范本中权衡，来得到特定分类的最后得分。在这个例子中，$h$ 是 $W_1$ 中每个范本的得分，$h$ 就像是得分函数，是 $W_1$ 里每个范本出现的得分，然后 $W_2$ 加权所有的中间变量的得分，来得到分类的最后得分。非线性通常出现在 $h$ 之前，所以 $h$ 是非线性之后的值。我们讲这些是为了直观的举例，在这个例子里，$W_1$ 像之前一样是用来寻找同样的模板，$W_2$ 是这些得分的加权。实际上会有一些出入。如你所说这里运用了非线性，但解释起来原理是接近的。$h$ 就是 $W_1$ 乘 $x$ 带 max 函数。

$W_1$ is directly connected to the input $x$, this is really interpretable, because you can formulate all of these templates. $h$ is a score of how much of each template you solve, for example, you have a 2 for the red car, 1 for the yellow car or something like that. In $W_1$ you have both left-facing horse and right-facing horse. So now $W_1$ can be many different kinds of templates. And $W_2$ is a weighted sum of all of these templates. So ow it allow you to weight together multiple templates in order to get the final score for a particular class. $h$ is the value, in this example, of scores for each of your templates that you have in $W_1$. $h$ is a score function, it's how much of each template in $W_1$ is present, and $W_2$ is going to weight all of these intermediate scores to get your final score for the class. The non-linearity usually happens right before $h$, so $h$ is the value right after the non-linearity. So we talking about this, like intuitively as this example of $W_1$ is looking for, has these same templates as before, $W_2$ is a weighting for these. In practice it's not exactly like this, because as you said, there's all these non-linearities thrown in and so on, but it has this approximate type of interpretation to it. $h$ is just $W_1$ times $x$, with the max function on top.

--------------------

<center>
    <image src = "./images/4.2_02_neural-networks2.png"></image>
</center>
-------------------------------------------------

我们用这个例子讲了两层神经网络，我们也可以堆更多层得到任意深度的神经网络。我们可以在非线性的条件下重复这一过程，乘以矩阵 $W_3$，就得到一个三层神经网络。这也就是深度神经网络的由来。思路是你可以堆很多层，得到深度网络。

So we've talked about this as an example of a two-layer neural network, and we can stack more layers of these to get deeper networks of arbitrary depth. So we can just do this one more time at another non-linearity and matrix multiply now by $W_3​$, and now we have a three-layer neural network. And this is where the term deep neural networks is coming from. The idea that you can stack multiple of these layers, for very deep networks.

现在我们已经了解了神经网络作为函数的原理。很多人都在谈论，神经网络如何从生物学中获得灵感，还是要强调一下，这个比喻是不严谨的，各种联系并不紧密。但了解这些联系和灵感还是挺有意思的。我简单说一下，说到神经元，脉冲信号传向各个神经元，很多神经元相互连接，每个神经元有多个树突，接收传给神经元的脉冲。然后细胞体，集中处理这些传入的信号，接收它， 然后整合再传出去，脉冲信号通过轴突从细胞体传向连接的下游神经元。现在回顾我们对每个计算节点，你会发现事实上，方法都是类似的。计算图里的节点相互连接，我们需要输入或者信号 $x​$ 传入神经元，所有 $x​$ 输入量 $x_0​$ $x_1​$ $x_1​$ 采用比如赋予权重 $W​$ 的方法。也就是说我们做了某种类型的运算，就像我们刚才做的那样，好比 $Wx+b​$，再把所有结果整合起来，我们得到一个激活函数，我们将激活函数应用在神经元的端部，得到的值作为输出，最后将该值传输到相关联的神经元。

We've sort of seen what neural networks are as a function. We hear people talking a lot about how there's biological inspirations for neural networks, and so even through it's import that to emphasize that these analogies are really loose, it's really just very loose ties, but it's still interesting to understand where some of these connections and inspirations come from. And so now I'm going to briefly about that. If we think about  a neuron, we have the impulses that are carried towards each neuron, so we have a lot of neurons connected together and each neuron has dendrites, and these are what receives the impulses that come into the neuron. And then we have a cell body, integrates these signals coming in and then there is a kind of , then it takes this, and after integrating all these signals, it passes on the impulses carries away from the cell body to downstream neurons that it's connected to, and it carries this away through axons. So now if we look at what we've been doing so far with each computational node, you can see that this actually has, you can see it in kind of a similar way. Where nodes are connected to each other in the computational graph, and we have inputs, or signal $x$, coming into a neuron, and then all of these $x$, $x_0$ $x_1$ $x_1$, these are combined and integrated together, using, for example our weights, $W$. So we do some sort of computation, and in some of the computations we've been doing so far, something like $Wx+b$, integrating all these together, and then we have an activation function that we apply on top, we get this value of this output, and we pass it down to the connecting neurons.

-----------------------

<center>
    <image src = "./images/4.2_03_neurons.png"></image>
</center>
---------------------------

再留意一下激活函数，它基本上接收了所有的输入然后再输出一个值。比如 sigmoid 激活函数，以及其他一些非线性函数，以及其它你能画出的任意类似项，这些非线性函数可以类比神经元的触发或者说神经元的放电率。神经元通过采用离散尖峰将信号传递至相邻神经元传递信号。如果触发非常迅速，然后通过神经元就会产生一类强信号。我们可以认为经过激活函数计算后的值，在某种意义上讲，我们将要传递下去的是放电率。实际上，我认为正在研究这些的神经科学家会说，其中一种非线性机制与神经元实际的生理行为最为相似，这种非线性函数就是 ReLU 非线性函数。如果输入是负值则输出为 0，输入值大于 0 时，则是一个线性函数。

If you look at these activation functions, this is what basically takes all the inputs coming in and outputs one number that's going out later on, and we've talked about examples like sigmoid activation function, and different kinds of non-linearities, and so sort of one kind of loose analogy that you can draw is that these non-linearities can represent something sort of like the firing, or spiking rate of the neurons. Where our neurons transmit signals to connecting neurons using kind of these discrete spikes, if they're spiking very fast then there's kind of a strong signal that's passed later on, and so we can think of this value after our activation function as sort of , in sense, sort of this firing rate that we're going to pass on. In practice, I think neuroscientists who are actually studying this say that kind of one of the non-linearities that are most similar to the way that neurons are actually behaving is a ReLU function, which is a ReLU non-linearity. It's a function that's a zero for all negative values of input, and then it's a linear function for positive regime. 

-------------

<center>
    <image src = "./images/4.2_04_neurons2.png"></image>
</center>
--------------------

然而需要注意的是，在进行这种类比时要特别小心，因为生物学上的神经元，实际上比我们刚才描述的要复杂得多。生物学上的神经元有很多不同种类，它们的树突会表现出异常复杂的非线性，而我们的突触，之前同 $W_0$ 进行类比的，并非像我们所描述的那样只有简单的权重，它们实际上非常复杂，是一个动态的非线性系统。另外我们将激活函数作为神经元的激活机制的类比也同样如此，实际上并不十分有效。生物学上的神经元的激发机制，用来描述神经元与后端神经元之间如何传递，可能不是一个有效模型，即使以一种简单的方式，神经元的激发阈值是有可能变化的，确实要考虑这种变化的可能性。像刚才说的，真实的神经元非常复杂，远没有我们之前处理的那么简单。

But it's really important to be extremely careful with making any of these sorts of brain analogies, because in practice biological neurons are way more complex than this. There is many different kinds of biological neurons, the dendrites can perform really complex non-linear computations. Our synapses, the $W_0​$ that we had earlier where we drew this analogy, are not single weights like we had, they are actually really complex, non-linear dynamical systems in practice. And also this idea of interpreting our activation function as a sort of rate code of firing rate is also insufficient in practice. It's just this kind of firing rate is probably not a sufficient model of how neurons will actually communicate to downstream neurons, like even as a very simple way, the neurons will fire at a variable rate, and this variability probably should be taken into account. And so there's all of these, it's kind of a much more complex thing than what we're dealing with.

--------------

<center>
    <image src = "./images/4.2_05_activation-functions.png"></image>
</center>

<center>
    <image src = "./images/4.2_06_architecture-of-neural-network.png"></image>
</center>
-------------------------

我们讨论了多种不同激活函数是怎么使用的，你可以看到刚才讨论的 ReLU 函数，之后会对所有的激活函数进行更加详细的讨论。针对你的需要去讨论如何选择激活函数，并且我们也会讨论神经元的不同架构形式。我们举例了这些全连接的神经网络，每一层都是矩阵相乘计算，我们实际上就想这么叫来着，之前我们叫的是两层神经元网络，具体来说我们有两个线性层，针对每个线性层我们做了一次矩阵乘法，我们称之为两个全连接层。我们也会成为单隐藏层神经网络，我们不是去计算做了多少次矩阵乘法，而是去数有多少隐藏层。你也可以用另一种叫法，我想两层神经网络这个叫法用得更多一些。右边这个神经网络，可以叫做三层神经网络或者双隐藏层神经网络。

So we talked about how there's many different kinds of activation functions that could be used, there's the ReLU that I mentioned earlier, and we'll talk about all of these different kinds of activation functions in much more detail later on, choices of these activation functions that you might want to use. And we'll also talk about different kinds of neural network architectures. So we give the example of these fully connected neural networks, where each layer is this matrix multiply, and so the way we actually want to call these is, we said two-layer neural network before, and that corresponded to the fact that we have two of these linear layers, where we're doing a matrix multiply, two fully connected layers is what we call these. We can also call this a one-hidden-layer neural network, so instead of counting the number of matrix multiplies we're doing, counting the number of hidden layers that we have. I think maybe two-layer neural network is something that's a little more commonly used. And then also here, for our three-layer neural network that we have, this can also called a two-hidden-layer neural network.

----------

<center>
    <image src = "./images/4.2_07_computation-of-feedforward-in-neural-network.png"></image>
</center>

<center>
    <image src = "./images/4.2_08_computation-of-feedforward-in-neural-network2.png"></image>
</center>
---------------------

当我们在做正向计算时，正向地通过神经网络，神经网络中的节点，主要进行神经元各种计算，就像我们刚才做的那样。实际情况是这样的，你可以认为那么一个隐藏层是一个向量，一组神经元的集合，就像这样写出来，利用矩阵乘法来计算神经元的值，针对整个神经元层，我们可以这样高效率地进行计算。所以通过一次矩阵乘法我们得到了该层所有神经元输出结果，比方说一层有 10 个神经元，50 个或者 100 个。再看一下，把它们写出来，所有按照矩阵或者向量的形式，针对非线性，$f$ 函数是我们要用的，在这里我们用 sigmoid 函数，我们可以把输入数据 $x$ 作为一个向量，这样我们就可以进行第一次矩阵乘法，$W_1$ 在前一步，之后用 $f$ 进行一步非线性计算，接着可以进行第二次矩阵乘法，可以得到第二个隐藏层，$h_2$，最后我们可以得到输出值。如果能做到这些，就可以去写神经网络了，像之前看到的逆推法，你只要用拟推算法计算就可以了，以上就是神经网络的基本应用过程了。

When we're doing this type of feed forward, forward pass through a neural network, each of the nodes in neural networks, is basically doing the kind of operation of the neuron the I showed earlier. So what's actually happening is each hidden layer you can think of as a whole vector, a set of these neurons, and so by writing it out this way with these matrix multiplies to compute our neuron values, it's a way that we can efficiently evaluate this entire layer of neurons. So with one matrix multiply we get output values of a layer of let's say 10, or 50 or 100 neurons. And so looking at this again, writing this out, all out in matrix form, matrix-vector form, we have non-linearity here. $F$ that we're using, in this case a sigmoid function, and we can take our data $x$, some input vector, and we can apply our first matrix multiply, $W_1$ on top of this, then our non-linearity, then a second matrix multiply to get a second hidden layer, $h_2$，and then we have our final output. This is basically all you need to be able to write a neural network, and as we saw earlier, the backward pass, you then just use backprop to compute all of those, and so that's basically all there is kind of the main idea of what's a neural network.

总结一下，针对全连接或者说线性层，我们讨论了如何将神经元组织起来进行运算。神经元层的抽象有一个好处，我们可以采用个非常高效的向量化代码去进行所有的运算。我们已经知道神经网络多重要，这是来自生物类比的灵感，但它们并不是真实的神经。只是做了个大概的比喻，下次我们将讨论卷积神经网络。

So just summarize, we talked about how we can arrange neurons into these computations of fully-connected or linear layers. This abstraction of a layer has a nice property that we can use very efficient vectorized code to compute all of these. We also talked about how it's important to keep in mid that neural networks do have some analogy and loose inspiration from biology, but they're not really neural. This is a pretty loose analogy that we're making, and next time we'll talk about convolutional neural networks.