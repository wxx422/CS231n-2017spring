# 3.2 优化 Optimization

##  3.2.1 介绍 Introduction

我们在做优化的时候，我通常想到在山谷里步行的事情。你在有不同山川、山谷和溪流的山谷里步行，这个景观的每一点，对应于参数 W 的一些设置，每个点的高度等于这个设定 W 所带来的损失。你就是在这个山谷里散步的人， 并且在努力寻找山谷的底部。

When we're doing optimization, I usually think of things in terms of walking round some large valley. So the idea is that you're walking around this large valley with different mountains and valleys and streams and stuff, and every point on this landscape corresponds to some setting of the parameters W, and the hight of each of these points, is equal to the loss incurred by that setting of W. And you're this little guy who's walking around this valley, and you're trying to find the bottom of this valley. This is a hard problem in general.

你可能会想到损失函数、正则化项的解析性质，也许可以写下最小化，这就相当于一路神奇地传送到谷底。但在实践中，一旦你的预测函数 f，损失函数和正则化项，一旦这些事情变得庞大而复杂，并且使用神经网络，写出来求解最小值的解析解的可能性很小。在实践中，我们倾向于从某个解决方案开始使用多种不同的迭代方法，然后逐步对它进行改进。

You might think about the analytic properties of loss function, regularization all that, maybe you can just write down the minimizer,  and that would sort of correspond to magically teleporting all the way to the bottom of this valley. But in practice, once your prediction function f, and your loss function and your regularizer, once these things get big and complex and using neural networks, there's really not much hope in trying to write down an explicit analytic solution that takes you directly to the minima.So in practice, we tend to use various types of iterative methods where we start with solution and then gradually improve it over time. 

最先想到的就是随机搜索，你需要很多权重值，随机采样，然后将它们输入损失函数，再看看效果怎样。但这是很烂的算法。

The first stupidest thing you might imagine is random search, that you'll just take a bunch of Ws, sampled randomly, and throw them into our loss function and see how well they do. But this is a really bad algorithm.

现实中，利用当地地形的几何形状可能是个更好的策略。当你一个人走在这样的地方，你可能不能看到一条直接的路线下到谷底。但你可以凭借脚上的感觉，感知当地地形的几何形状，哪条路可以往下走。你可以根据脚上的感觉，知道哪个方向地面的倾斜，是朝向下山的方向。然后朝那个方向走一步，你就下降了一点，再次用脚找到下山的方向，就这样一次次重复，最终希望能到底部。这看起来是一个简单的算法，但只要你能把每个细节做对，在实践中能得到很好的效果。这正是我们在训练大型神经网络、线性分类器和其他东西时，普遍使用的方法。

Maybe a better strategy is actually using some of the local geometry of this landscape. If you're this guy who's walking around this landscape, maybe you can't see directly the path down to the bottom of the valley, but  what you can do is feel with your foot and figure out what is the local geometry which way will take you a little bit downhill. So you can feel with your feet and feel where is the slope of the ground taking you down a little bit in this direction. And you can take a step in that direction, and you'll go down a little bit, feel again with your feet to figure out which way is down, and then repeat over and over again and hoper that you'll end up at the bottom of the valley eventually. This seems like a relatively simple algorithm, but actually this one tends to work really well in practice if you get all the details right. So this is generally the strategy that we'll follow when training these large neural networks and linear classifier and other things.

## 3.2.2 梯度下降 Gradient Descent

这只是一个简短的说明，那什么是斜率呢？如果还记得微积分，在一维上，斜率就是函数的导数。如果我们有一个一维函数，它将 x 作为标量，输出为曲线的高度，我们就能计算出任何一点的斜率，如果我们向任何方向前进一步 h，然后比较这一步前后函数值的差别，然后将步长趋于 0 的时候，我们就能得到那一点上函数的斜率。而且可以将它很自然地推广到多元函数。

That was a little hand wavy, so what is slope? If you remember calculus class, then at least in one dimension, the slope is the derivative of this function. So if we've got some one-dimensional function, f, that takes in a scalar x, and then outputs the height of some curve, then we can compute the slope or derivative at any point by imagining, if we take a small step, h, in any direction, and compare the difference in the function value over that step and drag that step size to zero, that would give us the slope of that function at that point. And this generalizes quite naturally to multi-variable function as well.

------

<center>
    <image src="./images/3.2_01_slope.png"></image>
</center>
------

在实际使用中，x 不是标量而是向量。所需我们需要有多元这个概念。在多元情况下生成的导数就叫做梯度。所以梯度就是偏导数组成的向量，梯度和 x 形状一样，梯度中的每个元素可以告诉我们相关方向上，函数 f 的斜率。正是梯度有这样优秀的特性，所以梯度就成为偏导数的向量，它指向函数增加最快的方向，相应地，负梯度方向就是指向了函数下降最快的方向。概括起来，如果想知道这个地形任意方向的斜率，它就等于这一点上梯度和该点单位方向向量的点积。所以梯度非常重要，因为它给出了函数在当前点的一阶线性逼近。所以实际操作中，很多深度学习都是在计算函数的梯度，然后用这些梯度迭代更新参数向量。

So in practice, our x is maybe not a scalar but a whole vector. So we need to generalize this notion to multi-variable things. And the generalization that we use of the derivative in the multi-variable setting is the gradient, so the gradient is a vector of partial derivatives. So the gradient will have the same shape as x, and each element of the gradient will tell us what is the slope of the function f, if we move in that coordinate direction. And the gradient turns out to have these very nice properties, so the gradient is now a vector of partial derivatives, but it points in the direction of greatest increase of the function and correspondingly, if you look at the negative gradient direction, that gives you the direction of greatest decrease of the function. And more generally, if you want to know, what is the slope of my landscape in any direction. Then that's equal to the dot product of the gradient with the unit vector describing that direction. So this gradient is super important, because it gives you this linear, first-order approximation to your function at your current point. So in practice, a lot of deep learning is about computing gradients of your functions and then using those gradients to iteratively update your parameter vector.

------

<center>
    <image src="./images/3.2_02_compute-gradient.png"></image>
</center>
------

在计算机上计算梯度的一个简单的方法，是有限差分法，这又回到了梯度的极限定义。左边，我们设想当前的 W 是这个向量参数，给了我们当前的损失函数比如说是 1.25，我们的目标是计算梯度 dW，它是和 W 相同维数的向量，梯度中每个元素都会告诉我们在相关方向上每移动一小步损失变化多少。所以你可能会猜想计算有限差分，只是将 W 的第一个元素累加一个很小的值，h，然后用我们的损失函数重新计算损失值以及我们的分类器。在这个设置中，如果我们在第一维做微小的改变，损失值将从 1.2534 降低为 1.25322。然后我们可以使用这个有限差分，在梯度的第一维，实现有限差分逼近。现在你可以想象，重新将第一维的值恢复为原值，在第二维重复这样的过程，在第二维方向上增加一小步。计算损失，使用有限差分逼近，在第二个维度上计算出梯度的近似值。再在第三个位置重复，如此反复。这也是一个可怕的想法，因为它非常慢。你可以想象，如果这个卷积神经网络非常大，计算这个函数 f 会很慢，这个参数向量，W，可能并不是像这里一样有 10 个输入，而是以千万计，在一些复杂的深度学习模型中，甚至是以亿计。所以在实际使用中，你绝不会想计算梯度的有限差分，因为你必须等待可能上亿次的函数估计来得到一个梯度，非常慢，结果也不好。

So one naive way that you might imagine evaluating this gradient on a computer, is using the method of finite differences, going back to the limit definition of gradient. So here on the left, we imagine that our current W is this parameter vector that gives us some current loss of 1.25 and our goal is to compute the gradient, dW, which will be a vector of the some shape as W, and each slot in that gradient will tell us how much will the loss change is we move a tiny, infinitesimal amount in that coordinate direction. So one thing that you might imagine is just computing these finite differences that we have our W, we might try to increment the first element of W by a small value, h, and then re-compute the loss using our loss function and our classifier and all that. And maybe in this setting, if we move a little bit in the first dimension, then our loss will decrease a little from 1.2534 to 1.25322. And then we can use this limit definition to come up with this finite differences approximation to the gradient in this first dimension. And now you can imagine repeating this procedure in the second dimension, where now we take the first dimension, set it back to the original value, and increment the second direction by a small step. And again, we compute the loss and use this finite differences approximation to compute an approximation to the gradient in the second slot. And now repeat this again for the third, and on and on and on. This is actually a terrible idea, because it's super slow. So you might imagine that computing this function, f, might actually be super slow if it's a large convolutional neural network. And this parameter vector, W, probably will not have 10 entries like it does here, it might have tens of millions or even hundreds of millions for some of these large, complex deep learning models. So in practice, you'll never want to compute your gradients for your finite differences, because you'd have to wait for hundreds of millions potentially of function evaluations to get a single gradient, and that would be super slow and super bad.

幸运的是你并不需要这样做，我们只需要写下损失的表达式然后用微积分这个工具，只需要写下梯度的表达式。比起使用有限差分法，对它进行计算分析，这将会很有效率。第一，它会非常精确，第二，我们只需要计算一个表达式，所以它会很快。现在如果我们回到当前的 W 这张图，他看起来是这样的，与迭代计算所有 W 维度不同，我们提前计算出梯度的表达式，然后将它写下来，从 W 值开始，计算 dW 或每步的梯度。在实际使用中，这种方法更好。

But thankfully we don't have to do that, we can just write down the expression for our loss and then use the magical hammer of calculus to just write down an expression for what is this gradient should be. And this'll be way more efficient than trying to compute it analytically via finite differences. One, it'll be exact, and two, it'll be much faster since we just need to compute this single expression. So what this would look like is now, if we go back to this picture of our current W, rather than iterating over all the dimensions of W, we'll figure out ahead of time what is the analytic expression for the gradient, and then just write it down and go directly from the W and compute the dW or the gradient in one step. And that would be much better in practice. 

------

<center>
    <image src="./images/3.2_03_summary-numeric-analytic.png"></image>
</center>
------

概括起来，在实际使用中你可能并不会使用数值梯度，但它很简单也很有意义。实际上，在计算梯度的时候，你将总是使用解析梯度。然而，有趣的是，数值梯度确实非常有用的调试工具。就是说当你已经写了一些代码，然后你再写一些代码来计算损失和梯度的损失，你怎样进行调试。你怎样验证写入代码的解析表达式是正确的。所以，通常的调试策略是使用数值梯度作为单元测试来确保你的解析梯度是正确的。当你使用数值梯度进行检查时，这个过程非常缓慢，并且不精确，你需要减少问题的参数数量，让它能在一个合理的时间内运行。但当你写你自己的梯度计算时，它确实是非常有用的调试策略。所以在实践中它非常常用。

So in summary, this numerical gradient is something that's simple and makes sense, but you won't really use it in practice. In practice, you'll always take an analytic gradient and use that when actually performing these gradient computations. How ever, one interesting note is that these numeric gradients are actually a very useful debugging tool. Say you've written some code, and you wrote some code that computes the loss and the gradient of the loss, then how do you debug this thing? How do you make sure that this analytic expression that you derived and wrote down in code is actually correct? So a really common debugging strategy for these things is to use the numeric gradient as a unit test to make sure that your analytic gradient was correct. Again, because this is super slow and inexact then when doing this numeric gradient checking as it's called, you'll tend to scale down the parameter of the problem so that it actually runs in a reasonable amount of time. But this ends up being a super useful debugging strategy when you're writing your own gradient computations. So this is actually very commonly used in practice.

```python
# Vanilla Gradient Descent
while True:
    weights_grad = evaluate_gradient(loss_fun, data, weights)
    weights += step_size * weights_grad # perform parameter update
```

所以一旦我们知道怎么计算梯度，它将我们引向一个超级简单，好像只需要三行代码的梯度下降算法，但事实上，它却是计算巨大也最为复杂的深度学习算法的核心。在梯度下降算法中，首先我们初始化 W 为随机值，当为真时，我们计算损失和梯度，然后向梯度相反的方向，更新权重值，因为，你需要记住，梯度是指向函数的最大增加方向，所以梯度下降则指向函数最大减小的方向，所以我们向梯度减小的方向向前进一小步，然后一直重复最后网络将会收敛。但是步长是一个超参数，告诉我们每次计算梯度时，在那个方向上前进多少距离。这个步长也叫作学习率，它可能是你需要设定的最重要的一个超参数。当我在训练网络时，确定这一步长，或者说学习率，是我要检查的第一个超参数。其他比如模型大小，需要多少正则化，我会晚点做，找到合适的学习率或步长是我首先要做的事情。

So once we know how to compute the gradient, then it leads us to this super simple algorithm that's like three lines, but turns out to be at the heart of how we train even these very biggest, most complex deep learning algorithms, and that's gradient descent. So gradient decent is first we initialize our W as some random thing, then while true, we'll compute our loss and our gradient and then we'll update our weights in the opposite of the gradient direction, because remember that the gradient was pointing in the direction of greatest increase of the function, so minus gradient points in the direction of greatest decrease,  so we'll take a small step in the direction of minus gradient, and just repeat this forever and eventually your network will converge and you'll be very happy, hopefully. But this step size is actually a hyper-parameter, and this tells us that every time we compute the gradient, how far do we step in that direction. And this step size, also sometimes called a learning rate, is probably one of the single most important hyper-parameter that you need to set when you're actually training these things in practice. Actually for me when I'm training these things, trying to figure out this step size or this learning rate, is the first hyper-parameter that I always check. Things like model size or regularization strength, I leave until a little bit later, and getting the learning rate or the step size correct is the first thing that I try to set at the beginning.

------

<center>
    <image src="./images/3.2_04_bowl-gradient.png"></image>
</center>
------

如图所示，这是一张 2 维示意图。这里我们有一个碗形，表示我们的误差函数，中心的红色区域，表示误差值较低，是我们的目标，这些靠近边缘的蓝色和绿色区域，代表较高的误差值，使我们要避免的，现在我们以一个空间内的随机点来开始我们的 W，然后计算梯度的反方向，我们希望它将指向最终的最小值，如果我们重复这一过程，那最终，我们会达到最小值，

So pictorially what this looks like, here's a simple example in two dimensions, we've got this bowl that showing our loss function where this red region in the center is this region of low loss we want to get to and  these blue and green regions towards the edge are higher loss that we want to avoid. So now, we're going to start of our W at some random point in the space, and then we'll compute the negative gradient direction, which will hopefully point us in the direction of the minima eventually. And if we repeat this over and over again, we'll hopefully eventually get to the exact minima.

## 3.2.3 随机梯度下降 Stochastic Gradient Descent 

------

<center>
    <image src="./images/3.2_05_SGD.png"></image>
</center>
------

记得我们已经定义了误差函数，我们定义了一种损失误差计算我们的分类器在训练样本的每一步表现有多糟糕，然后我们设定数据集的总误差是整个训练集误差的平均值。但实际中，N 可能会非常大，如果用的是 ImageNet 数据库，N 可能会大至 130 万。所以实际上要计算这个 loss 值，计算成本会非常高，可能对这一函数需要上百万次的计算。因为梯度是一个线性运算，当你试着计算表达式的梯度时，你会发现，误差函数的梯度值，是每个单项误差梯度的总和。所以，如果我们要再次计算梯度，就需要我们迭代整个训练数据集的所有 N 个样本。实际应用中，我们使用随机梯度下降，它并非计算整个训练集的误差和梯度值，而在每一次迭代中，选取一小部分训练样本，称为 minibatch。通常，取 2 的 n 次方，如 32，64，128  是常用的数字，然后我们利用 minibatch 来估算误差总和以及实际梯度。你可以把它当做对真实数值期望的一种蒙特卡洛估计，这是随机的。

Remember we defined our loss function, we defined a loss that computes how bad is our classifier doing at any single training example,  and then we said that our full loss over the data set was going to be the average loss across the entire training set. But in practice, this N could be very large, if you're using the ImageNet data set for example, then N could be like 1.3 million. So actually computing this loss, could be very expensive and require computing perhaps millions of evaluations of this function. And because gradient is a linear operator, when you actually try to compute the gradient of this expression, you see that the gradient of our loss is now the sum of the gradient of the losses for each of the individual terms. So now, if we want to compute the gradient again, it sort of requires us to iterate over the entire training data set all N of these examples. So in practice, we tend to use what is called stochastic gradient descent, where rather than computing the loss and gradient over the entire training set, instead at every iteration, we sample some small set of training examples, called a minibatch. Usually this is a power of two by convention, like 32, 64, 128 are common numbers, and then we'll use this small minibatch to compute an estimate of the full sum, and an estimate of the true gradient. And this is stochastic because you can view this as a Monte Carlo estimate of some expectation of the true value. 
```python
# Vanilla Minibatch Gradient Descent
while True:
    data_batch = sample_training_data(data, 256) # sample 256 examples
    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
    weights += step_size * weights_grad # perform parameter update
```
这就让我们的算法稍微高级一点，但仍然只要 4 行。当为真时，随机取 minibatch 数据，评估 minibatch 的误差值和梯度，然后基于这一误差值和梯度的估计更新各参数。我们会看到稍微高级一点的更新规则，在过程中如何整合多个梯度，但这是一个我们在深度神经网络中经常用到的基本训练算法。

So now this makes our algorithm slightly fancier, but it's still only four lines. While true, sample some random minibatch of data, evaluate your loss and gradient on the minibatch, and now make an update on your parameters based on this estimate of the loss and this estimate of the gradient. We'll see sightly fancier update rules of exactly how to integrate multiple gradients over time, but this is the basic training algorithm that we use for pretty much all deep neural networks in practice.

## 3.2.4 图像特征 Image Feature

接下来说的是另一个概念，是图像的特征。我们已经讲了线性分类器，就是将我们的原始像素取出，将这些原始像素直接传入线性分类器。但如上节课所讲，因为多模态之类的原因，这样做不太好。所以实际操作中，如果直接输入原始像素值，传递给线性分类器，表现不好。所以，当深度神经网络大规模运用之前，常用的方式就是使用两步走策略，首先，拿到图片计算图片的各种特征代表，例如，可能计算与图片形象有关的数值，然后，将不同的特征向量合到一起，得到图像的特征表述。这个关于图像的特征表述，会作为输入源传入线性分类器，而不是将原始像素传入分类器。

Now as an aside, I'd like to talk about another idea, which is that of image features. So far, we've talked about linear classifiers, which is just taking our raw images pixels and then feeding the raw pixels themselves into our linear classifier. But as we talked about in the last lecture,  this is not such a great thing to do, because of things like multi-modality and what not. So in practice, actually feeding raw pixel values into linear classifiers tends to not work so well. So it was actually common before the dominance of deep neural networks, was instead to have this two-stage approach, where first, you would take your image and then compute various feature representations of that image, that are maybe computing different kinds of quantities relating to the appearance of the image, and then concatenate these different feature vectors to get you some feature representation of the image, and now this feature representation of the image would be fed into a linear classifier, rather than feeding the raw pixels themselves into the classifier.

------

<center>
    <image src="./images/3.2_06_polar-coordinates.png"></image>
</center>
------

这么做的动机是什么？想象一下我们有一个训练数据集如左侧所示，红点分布在中间，蓝点分布在周围。对于这种类型的数据集，我们不能用一个线性决策边界，将红点从蓝点中分开。但是如果我们采用一个灵活的特征转换，在这个例子中我们运用极坐标转换，现在我们得到转换特征，就可以把复杂的数据集变成线性可分的，然后可以用线性分类器正确分类。这里我们整个策略就是找到正确的特征转换，从而计算出所关心问题的正确指标。对图像而言，将原始像素转换到极坐标并不能起什么作用，但事实上，你可以记下有可能起作用的图像特征表示。这可能会比将原始像素放入分类器具有更好的效果。

And what is the motivation here? Imagine we have a training data set /.on the left, and red points in the middle and blue points around that. And for this kind of data set, there's no way that we can draw a linear decision boundary to separate the red points from the blue points. But if we use a clever feature transform, in this case transforming to polar coordinates, then now after we do the feature transform, then this complex data set actually might become linearly separable and could be classified correctly by a linear classifier. And the whole trick here now is to figure out what is the right feature transform that is computing the right quantities for the problem that you care about. So for images, converting your pixels to polar coordinates, doesn't make sense, but you actually can try to write down feature representations of images that might make sense. It might do better than putting in raw pixels into the classifier.

------

<center>
    <image src = "./images/3.2_07_color-histogram.png"></image>
</center>
------

一个特征表示非常简单的例子就是颜色直方图。获取对应的光谱，把它分到桶里，将每一个像素都映射到这些桶里，然后计算出每一个不同桶里像素点出现的频次。这个从全局上告诉我们图像中有哪些颜色。比如这里的青蛙，特征向量告诉我们这里有很多绿色分量没有太多紫色和红色分量。你可以在实践中发现这种简单的特征向量。

One example of this kind of feature representation that's super simple, is this idea of a color histogram. You'll take this hue color spectrum and divide it into buckets and then for every pixel, you'll map it into one of those color buckets and then count up how many pixels fall into each of these different buckets. So this tells you globally what colors are in the image. This example of a frog, this feature vector would tell us there's a lot of green stuff, maybe not a lot of purple or red stuff. And this is a simple feature vector that you might see in practice.

------

<center>
    <image src = "./images/3.2_08_HoG.png"></image>
</center>
------

在神经网络兴起之前，或者说神经网络确立统治地位之前，一个常用的特征向量就是方向梯度直方图。Hubel 和 Wiesel 发现人类视觉系统中，有向边缘非常重要，而特征表示的有向梯度直方图尝试获取同样的感觉，并且测量图像中边缘的局部方向。这个特征向量首先获取图像然后将图像按八个像素区分成八份，然后在八个像素区的每一个部分计算每个像素值的主要边缘方向。把这些边缘方向量化到几个组，然后在每一个区域内，计算不同的边缘方向从而得到一个直方图。现在，你的全特征向量就是这些不同组的边缘方向直方图，这个直方图是从图像的八个区域得来的。在某种程度上，和我们之前看到的颜色直方图分类器类似，颜色直方图从全局上看出图像中存在什么颜色，这里是说，图像中存在什么类型的边缘。即使是图像的不同部分，不同区域内存在着哪些类型的边缘都可以知道。比如左边的青蛙坐在叶子上，叶子主要存在对角线边缘，如果将这些方向梯度特征的直方图可视化，可以看到在这个区域里，这就是获取的方向梯度特征表示的直方图。这是一个非常常见的用于目标识别的特征表示。

Another common feature vector that we saw before the rise of neural networks or before the dominance of neural networks was this histogram of oriented gradients. Hubel and Wiesel found these oriented edges are really important in the human visual system, and this histogram of oriented gradients feature representation tries to capture the same intuition and measure the local orientation of edges on the image. So what this thing is going to do, is take our image and then divide it into these little eight pixel regions. And then within each of those eight by eight pixel regions, we'll compute what is the dominant edge direction of each pixel, quantize those edge directions into several buckets and then within each of those regions compute a histogram over these different edge orientations. And now your full-feature vector will be these different bucketed histograms of edge orientations across all the different eight by eight regions in the image. So this is in some sense dual to the histogram classifier that we saw before. Color histogram is saying, globally what colors exist in the image and this is saying, overall, what types of edge information exist in the image. And even localized to different parts of the image, what types of edges exist in different regions. For this frog on the left, you can see it's sitting on a leaf, and these leaves have these dominant diagonal edges, and if you visualize the histogram of oriented gradient features, then you can see that in this region, we've got a lot of diagonal edges, that this histogram of oriented gradient feature representation's capturing. So this was a super common feature representation and was used a lot for object recognition.

------

<center>
    <image src = "./images/3.2_09_image-vocabulary.png"></image>
</center>
------

另一个特征表示的例子就是词袋。这是从自然语言处理中获得的灵感。如果你得到一段话，用特征向量表示这段话的一个方法就是计算不同词在这段话中出现的频次。我们想得到这种直觉，并且以某种方式应用于图像，但问题是，简单地把文字类比于图像并不容易，因此我们需要定义自己的视觉单词字典。我们采用两阶段方法，首先获得一堆图像，从这些图像中进行小的随机块的采样，然后用 K 均值等方法将他们聚合成簇，从而得到不同的簇中心，这些簇中心可能代表了图像中视觉单词的不同类型。看看现在这个例子，这是聚类的真实例子，从图像中得到不同的图像块，你可以发现，聚类操作之后，我们的视觉单词获取了不同的颜色，比如红、蓝、黄，就像不同方向有向边缘的不同类型，这很有意思。现在我们看看数据驱动方式下，从数据中获得这些有向边缘。一旦我们获得一系列视觉单词，也叫码本，我们就可以用这些视觉单词来给图像进行编码，这个视觉单词在图像中出现多少次。这再次给了我们关于图像的视觉外观一些不同的信息。实际上这是李飞飞在她研究生期间从事的一种特征表示。

Another feature representation that you might see out there is this idea of bag of words. Here is taking inspiration from natural language processing. If you've got a paragraph then a way that you might represent a paragraph by a feature vector is counting up the occurrences of different words in that paragraph. So we want to take that intuition and apply it to images in some way. But the problem is that there's no really simple, straightforward analogy of words to images, so we need to define our own vocabulary of visual words. We take this two-stage approach, where first we'll get a bunch of images, sample a whole bunch of tiny random crops from those images and then cluster them using something like K means to come up with these different cluster centers that are representing different types of visual words in the images. If you look at this example on the right here, this is real example of clustering different image patches from images, and you can see that, after this clustering step, our visual words capture these different colors, like red and blue and yellow, as well as these different types of oriented edges in different directions, which is interesting that now we're starting to see these oriented edges come out from the data in a data-driven way. Once we've got these set of visual words, also called a codebook, then we can encode our image by trying to say, for each of these visual words, how much does this visual word occur in the image? And now this gives us, again, some slightly different information about what is the visual appearance of this image. And actually this is a type of feature representation that Fei-Fei worked on when she was a grad student, so this is something that you saw in practice not too long ago.

------

<center>
    <image src = "./images/3.2_10_feature-vs-convolutional.png"></image>
</center>
------

作为一个引子，把全部都试一试， 5 到 10 年前图像分类通道就像这样，拿出你的图像，然后计算图像不同特征表示的差异，比如词袋或者方向梯度的直方图，将整个特征连接在一起，来喂养这些线性分类器的特征提取器。这种方式会比那样更复杂一些，但这是更常见的思路。接下来的想法就是在提取这些特征后，固定特征提取器，使得它在训练中不会被更新。在训练中，如果在特征上可用，仅仅更新线性分类器。事实上我认为，一旦我们移到卷积神经网络和深度神经网络，看上去也没有多少不同。唯一的差别就是，并非提前记录特征，而是直接从数据中学习特征。所以我们将获取的像素值输入卷积神经网络，经过多层计算，最终得到一些数据驱动的特征表示的类型，然后我们在整个网络中训练所有的权重，而不是最上层线性分类器的权重。

So then as a bit of teaser, trying this all back together, the way that this image classification pipeline might have looked like, maybe about 5 to 10 years ago, would be that you would take your image, and then compute these different feature representations of your image, things like bag of words, or histogram of orientated gradients, concatenate a whole bunch of features together, and then feed these feature extractors down into some linear classifier. The pipelines were a little bit more complex than that, but this is the general intuition. And then the idea here was that after you extracted these features, this feature extractor would be a fixed block that would not be updated during training. And during training, you would only upgrade the linear classifier if it's working on top of features. And actually, I would argue that once we move to convolutional neural networks, and these deep neural networks, it actually doesn't look that different. The only difference is that rather than writing down the features ahead of time, we're going to learn the features directly from the data. So we'll take our raw pixels and feed them into this to convolutional networks, which will end up computing through many different layers, some type of feature representation driven by the data, and then we'll actually train this entire weights for this entire network,rather than just the weights of linear classifier on top.

 