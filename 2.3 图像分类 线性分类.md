# 2.3 图像分类 线性分类 Linear Classification

##  2.3.1 介绍 Introduction

线性分类相对简单，但是一个非常重要的学习算法。这有助于我们建立起整个神经网络和卷积网络。我们经常把神经网络比喻为玩乐高。你可以有不同种类的神经网络组件，你可以将这些组件组合在一起，来构建不同的大型卷积网络。它是我们在不同类型的深度学习应用程序中，看到的最基本的构件块之一。所以很好地了解线性分类器工作原理是非常重要的。因为这些将最终完全泛化到整个神经网络。

Linear classification is quite a simple learning algorithm, but this will become super important and help us build up to whole neural networks and whole convolutional networks. One analogy people often talk about when working with neural networks is we think of them as being kind of like Lego blocks. That you can have different kinds of components of neural networks and you can stick these components together to build these large different towers of convolutional networks. One of the most basic building blocks that we'll see in different types of deep learning applications is this linear classifier. So I think it's actually really important to have a good understanding of what's happening with linear classification. Because these will end up generalizing quite nicely to whole neural networks.

这类有关神经网络的模块化性质的另一个例子来自于我们自己的图像字幕实验室的一些研究，就像预览一样。我们计划输入一副图像，然后输出用于描述图像的描述性句子。这类工作的方法是卷积神经网络只关注图像，而循环神经网络只关注语言。而且我们可以把这两个网络放在一起，就像玩乐高一样，再将其一起训练，最终得到一个超级厉害的系统，来做一些伟大的事情。我们将会在课堂上详细介绍这个模型，但这只是能让你感到这些深层神经网络就像是乐高玩具，而这种线性分类器就像巨型网络的基础模块。这是这对于第二节课来说太刺激了，所以现在我们必须先回到 CIFAR-10。

So another example of kind of this modular nature of neural network comes from some research in our own lab on image captioning just as a little bit of a preview. Here the setup is that we want to input an image, and then output a descriptive sentence describing the image. And the way this kind of works is that we have one convolutional neural network that's looking at the image, and a recurrent neural network that knows about language. And we can just stick these two pieces together, like Lego blocks and train the whole thing together and end up with a pretty cool system that can do some non-trivial things. And we'll work through the details of this model as we go forward in the class, but this just gives you the sense that, these deep neural networks are kind of like Legos and this linear classifier is kind of like the most basic building blocks of these giant networks. But that's a little bit too exciting fir lecture two, so we have to go back to CIFAR-10 for the moment.

## 2.3.2 线性分类器 Linear Classification

回想一下，在 CIFAR-10 中有 5 万个训练样例，每个图像都是 32 x 32 像素的图像，同时每幅图像有 3 个色彩通道。我们采用与 k-最近邻 不同的方法。线性分类器是参数模型中最简单的例子，我们的参数模型有两个不同的部分。输入一张比如左边的猫的图片，我们通常用 X 表示输入数据，还有一组参数或者说权重，通常叫做 W，有时也叫 θ，文献不同叫法也不同。现在我们要写一些函数包含输入数据 X 和参数 W，这就会出现 10 个数字来描述在 CIFAR-10 中对应的 10 个类别所对应的分数。根据上述表述，比如说猫的分数更大，表明输入 X 是猫的可能性更大。

Recall that CIFAR-10 has these 50,000 training examples, each image is 32 by 32 pixels and three color channels. In linear classification, we're going to take a bit of a different approach from k-nearest neighbor. The linear classifier is one of the simplest examples of what we call a parametric model. Our parametric model actually has two different components. It's going to take in this image of a cat on the left, we usually write as X for our input data, and also a set of parameters, or weights, which is usually called W, also sometimes theta, depending on the literature. And now we're going to write down some function which takes in both the data, X, and the parameters, W, and this'll spit out now 10 numbers describing what are these scores corresponding to each of those 10 categories in CIFAR-10. With the interpretation that, like the larger score for cat, indicates a larger probability of that input X being cat.

在 K-最近邻 算法的设置中没有参数，取而代之的是我们通常会保留所有种类的训练数据，并在测试时使用。但在参数化方法中，我们姜总接我们对训练数据的认识并把所有的知识都用到这些参数 W 中。在测试时，我们不再需要实际的训练数据，可以扔掉。在测试时，只需要这些参数 W。这使得我们的模型更加高效，甚至可以运行在手机这样的小设备上。在深度学习中，整个描述都是关于函数 F 正确的结构。你可以想象编写不同的函数形式，用不同的、复杂的方式组合权重和数据，这些可以对应于不同的神经网络体系结构。但将它们相乘可能是最简单的结合方式。这就是线性分类器。

In the k-nearest neighbor setup there was no parameters, instead, we just kind of keep around the whole training data, and use that at test time. But now, in a parametric approach, we're going to summarize our knowledge of the training data and stick all that knowledge into these parameters, W. And at test time, we no longer need the actual training data, we can throw it away. We only need these parameters, W, at test time. So this allows our models to now be more efficient and run on small devices like phones. The whole story in deep learning is coming up with the right structure for this function, F. You can imagine writing down different functional forms for how to combine weights and data in different complex ways, and these could correspond to different network architectures. But the simplest possible example of combining these two things is just, maybe, to multiply them. And this is a linear classifier.

------

<center>
    <image src="./images/2.3_01_linear-calssification-function.png"></image>
</center>
------

所以 F(x) = W \* X，这可能是你能想到的最简单的方程。如果把上述方程的维度解出来，前提是图像是 32 x 32 x 3 的输入值。我们要取这些值然后把它们展开成一个 3072 项的长列向量。现在我们想要得出这张图像对应的 10 个分类的得分。也就是说，现在我们的 W 是 10 \* 3072 项的矩阵，X 是 3072 \* 10 项的矩阵。因此，一旦我们将这两项相乘，我们就会得到一个列向量给我们 10 个类的分数。有时，你会看到我们通常会添加一个偏置项，一个 10 元素的常熟向量，它不与训练数据交互，而只会给我们一些数据独立的偏好值，针对某些类的偏好值。如果你的数据是不平衡的，比如猫比狗多，那么与猫对应的偏差元素就会比其他的要高。

So here our F of X, is just equal to the W times X. Probably the simplest equation you can imagine. If you unpack the dimensions of these things, we recall that our image was 32 by 32 by 3 values. We're going to take those values and then stretch them out into a long column vector that has 3,072 by one entries. And now we want to end up with 10 class scores for this image. Which means that now our matrix, W, needs to be 10 by 3072. So that once we multiply these two things out then we'll end up with a single column vector 10 by 1, giving us our 10 class scores. Also sometimes, you'll typically see this, we'll often add a bias term which will be a constant vector of 10 elements that doesn't interact with the training data, and instead just gives us some sort of data independent preferences for some classes over another. So you might imagine that if your dataset is unbalanced and had many more cats than dogs, then the bias elements corresponding to cat would be higher than the other ones.

------

<center>
    <image src="./images/2.3_02_linear-calssification-computing.png"></image>
</center>
------

我们有一个例子，左边是一个简单的 2 \* 2 图像，有 4 个像素。所以线性分类器的工作方式是，我们把这 2 \* 2 图像拉伸成一个有 4 个元素的列向量。在这个例子中，我们只是限制三类，猫、狗和船。现在权重矩阵是  3 \* 4。我们有 4 个像素和 3 个类，现在又有了一个 3 元素的偏置向量。我们看到，猫的分数将会是我们图像的像素和权重矩阵这行之间的乘积再加上这个偏置项。当你这样看待它的时候，你就可以**理解线性分类几乎是一种模板匹配方法**。这个矩阵的每一行对应图像的某个模板。计算矩阵行和图像像素值拉伸的列向量的乘积，来我们发现，这个类的模板和图像的像素之间有相似之处。然后偏置项给了这个数据独立缩放比例以及每个类的偏移量。

In this figure we have an example on the left, of a simple image with just a 2 by 2 image, so it has 4 pixels. So the way the linear classifier works is that we take this 2 by 2 image, we stretch it out into a column vector with 4 elements, and now, in this example, we are just restricting to three classes, cat, dog, and ship. And now our weight matrix is going to be three by four ,  we have 4 pixels and 3 classes. And now, again, we have a three element bias vector that gives us data independent bias terms for each category. Now we see the cat score is going to be the enter product between the pixels of our image and this row in the weight matrix added together with this bias term. When you look at it this way, you can kind of understand **linear classification as almost a template matching approach**. Where each of the rows in this matrix correspond to some template of the image. And now, the enter product or dot product between the row of the matrix and the column giving the pixels of the image, computing this dot product kind of gives us a similarity between this template for the class and the pixels of our image. And then bias just gives you this data independence scaling offset to each of the classes.

如果我们根据这个模板匹配的观点，考虑线性分类器，我们可以去权重矩阵的行向量并且将它们还原回图像，将这些模板可视化成图像。这给我们展示了一个线性分类器实际上在尝试理解我们的数据是怎样做的。

If we think about linear classification from this viewpoint of template matching, we can take the rows of that weight matrix and unravel them back into images and visualize those templates as images. And this gives us some sense of what a linear classifier might be doing to try to understand our data.

------

<center>
    <image src = "./images/2.3_03_linear-calssification-interpreting.png"></image>
</center>
------

在这个例子中，我们已经提前在图像上训练好了一个线性分类器。下面就是我们数据集中训练得到的权重矩阵中的行向量对应于 10 个类别相关的可视化效果。这样，看我们就能了解到在这些图像上发生了什么。举例来说，在左下角，我们看到飞机类别的模板，似乎由中间类似蓝色斑点状图形，和蓝色背景组成。这样就产生一种感觉，你会认为飞机的线性分类器可能是在寻找蓝色图形和斑点状图形，然后这些特征使得这个分类器更像飞机。我们看这个汽车的例子，我们能看到在中间有一个红色斑点状物体以及在顶部是一个或许是挡风玻璃的蓝色斑点状物体。但这有些奇怪，这样看起来真的不像一辆车。没有单独的汽车是这样的。所以就存在这样一个问题，线性分类器每个类别只能学习一个模板。如果这个类别出现了某种类型的变体，那么它将尝试求取所有不同变体的平均值。并且只使用一个单独的模板来识别其中的每一个类别。我们也可以在马的分类器中明显看到。因为马通常站在草地上，在马匹分类器中，我们看到下方是一片绿色，仔细看会发现这个马实际上看起来有两个头，一边一个。线性分类器只是尽其所能，因为它只允许学习每个类别的一个模板。当我们来看神经网络和更复杂的模型，我们能得到更好的准确率，因为这些模型再也没有只能每个类别学习一个单独的模板的限制。

In this example, we've gone ahead and trained a linear classifier on our images. And the bottom we're visualizing what are those rows in that learned weight matrix corresponding to each of the 10 categories in CIFAR-10. And in this way we kind of get a sense for what's going on in these images. For example, on the bottom left, we see the template for the plane class, kind of consist of this like blue blob, this kind of blobby thing in the middle and maybe blue in the background, which gives you the sense that this linear classifier for plane is maybe looking for blue stuff and blobby stuff, and those features are going to cause the classifier to like planes more. If we looking at this car example, we kind of see that there's a red blobby thing through the middle and a blue blobby thing at the top that maybe is kind of a blurry windshield. But this is a little bit weired, this doesn't really look a car, no individual cat actually looks like this. So the problem is that the linear classifier is only learning one template for each class. So if there's sort of variation in how that class might appear, it's trying to average out all those different variations, all those different appearances, and use just one single template to recognize each of those categories. We can also see this pretty explicitly in the horse classifier. In the horse classifier we see green stuff on the bottom because horses are usually on grass. If you look carefully, the horse actually seems to have maybe two heads, one head on each side. The linear classifier is just doing the best that it can, because it's only allowed to learn one template per category. And as we move forward into neural networks and more complex models, we'll be able to achieve much better accuracy because they no longer have this restriction of just learning a single template per category.

------

<center>
    <image src = "./images/2.3_04_linear-calssification-interpreting2.png"></image>
</center>
------

线性分类器的另一个观点是回归到图像，作为点和高维空间的概念。你可以想象每一张图片都是类似高维空间中的一个点的东西。现在这个线性分类器在这些先行决策边界上尝试画一个线性分类面来划分一个类别和剩余其他类别。在左上角我们看到用来训练的飞机样例，通过训练过程，这个线性分类器会尝试绘制这条蓝色直线把飞机这个类别划分出来。如果你在训练过程中注意观察，这些线条会随机地开始然后快速变化，试图将数据正确区分开。

Another viewpoint of the linear classifier is go back to this idea of image as points and high dimensional space. And you can imagine that each of our images is something like a point in this high dimensional space. And now the linear classifier is putting in these linear decision boundaries to try to draw linear separation between one category and the rest of the categories. Up on the upper-left hand side, we see these training examples of airplanes and throughout the process of training the linear classifier will go and try to draw this blue line to separate out with a single line the airplane class all the rest of the classes. And it's kind of fun if you watch during the process these lines will start out randomly and then go and snap into place to try to separate the data properly. 

## 2.3.3 缺陷 Defect

------

<center>
    <image src="./images/2.3_05_hard-cases-for-linear-classifier.png"></image>
</center>
------

当你从高维空间的角度来考虑线性分类器，你就能再次看到线性分类器中可能会出现的问题。要构造一个线性分类器完全失效的数据集样例并不难。假设我们有一个两个类别的数据集，蓝色和红色。蓝色类别是图像中像素的数量，数字大于 0，且是奇数。任何像素个数大于 0 的图像都归为红色类别。可以看到奇数像素点的蓝色类别在平面上有两个象限，甚至是两个相反的象限。我们没有办法能够绘制一条单独的直线来划分蓝色和红色，这是线性分类器的困境。也许归根结底这不是人工数据，实际上是在计算图像中动物或人类的奇偶数，而非像素点。所以这种划分奇数偶数的问题，是线性分类器通过传统方法难以解决的。

When you think about linear classification in this way, from this high dimensional point of view, you can start to see again what are some of the problems that might come up with linear classification. And it's not too hard to construct examples of datasets where a linear classifier will totally fail. Suppose we have a dataset of two categories, blue and red. And the blue categories are the number of pixels in the image, which are greater than zero, is odd. And anything where the number of pixels greater than zero is even, we want to classify as the red category. If you go and draw what these different decisions regions look like in the plane, you can see that our blue class with an odd number of pixels is going to be these two quadrants in the plane, and even will be the opposite two quadrants. There's no way that we can draw a single linear line to separate the blue from the red. So this would be an example where a linear classifier would really struggle. And this is not such an artificial thing after all. Instead of counting pixels, maybe we're trying to count whether the number of animals or people in an image is odd or even. So this kind of a parity problem of separating odds from evens is something that linear classification really struggle with traditionally.

线性分类器难以解决的其他情况是多分类问题。我们蓝色类别存在于三个不同象限，然后其他所有都是另一个类别。对于我们在之前的例子中看到的像马匹这样的东西，在实际情况下会出现。在马匹像素空间中，可能有一个头向左看，另一个头向右看。目前还没有好办法可以在这两个类别之间绘制一条单独的线性边界。所以，任何时候当你有多模态数据，比如一个类别出现不同的领域空间，这是另一个线性分类器可能困顿的地方。线性分类器存在很多问题，但是它是一个超级简单的算法，易于使用和理解。

Other situations where a linear classifier really struggles are multimodal situations. Our blue category has three different islands of where the blue category lives, and then everything else is some other category. For something like horses, we saw on the previous example, is something where this actually might be happening in practice. Where there's maybe one island in the pixel space of horse looking to the left, and the other island of horses looking to the right. And now there's no good way to draw a single linear boundary between these two isolated islands of data. So anytime where you have multimodal data like one class that can appear in different regions of space, is another place where linear classifiers might struggle. There is kind of a lot problems with linear classifiers, but it is a super simple algorithm, super nice and easy to interpret and easy to understand.

## 2.3.4 总结 Summary

我们讨论了线性分类器对应的函数形式。这种矩阵向量相乘的函数形式对应于模板匹配，在数据集中为每一个类别学习一个单独的模板。一旦我们有了这个训练矩阵，你可以使用它的得到任何新的训练样本的得分。

We've talked about the functional form corresponding to a linear classifier. This functional form of matrix vector multiply corresponds this idea of template matching and learning a single template for each category in your data. Then once we have this trained matrix, you can use it to actually go and get your scores for any new training example.

线性分类器可以解释为每个种类的学习模板，对图像中的每一个像素点以及 10 个分类里的每一项，矩阵 W 里都有一些对应的项，告诉我们那个像素对那个分类有多少影响。也就是说矩阵 W 的每一行都对应一个分类模板，每一行又分别对应一些权重，每个图像像素值和对应那个类别的一些权重，如果我们将这行分解回图像的大小，我们就可以可视化地观察到学习到的每个类的模板。还有一种对线性分类器的解释是学习像素在高维空间的一个现行决策边界，其中高维空间就对应了图片能渠道的像素密度值。

Linear classification has this interpretation as learning templates per class,  for every pixel in the image and for every one of our 10 classes, there exists some entry in this matrix W, telling us how much does that pixel influence that class. So that means that each of these rows in the matrix W ends up corresponding to a template for the class. Each of rows corresponds to a weighting between the pixel values of the image and that class, if we take those rows and unravel it back into an image, then we can visualize the learned template for each of these classes. We also had this interpretation of linear classification as learning linear decision boundaries between pixels in some high dimensional space where the dimensions of the space correspond to the values of the pixel intensity values of the image.

 但在课程中没有讲述的是如何给你的数据集选择一个正确的权重矩阵 W。我们只是讨论了函数形式是什么以及这是怎么回事。下节课我们将介绍如何选择正确权重的策略和算法。也就是损失函数和优化以及卷积方面的问题。

But what we have not told you is how do you actually go about choosing the right W for you dataset. We just talked about what is the functional form and what is going on with this thing. Next we'll talk about what are the strategies and algorithms for choosing the right W. And this will lead us the questions of loss functions and optimization and eventually ConvNets.



