# 4.1 反向传播 Backpropagation 

##  4.1.1 回顾和简介 Recap and Brief Introduction

------

<center>
    <image src="./images/4.1_01_recap.png"></image>
</center>
------

我们已经讲了怎样用函数 $f$ 定义一个分类器，函数 $f$ 的参数是权重矩阵 $W$，输入数据 $x$ 并对想要分类的每个类别都输出一个对应的得分向量。由此我们还可以定义一个损失函数比如 SVM 损失函数，这个函数基础量化了我们对模型预测结果满意或不满意的程度。然后我们可以用它定义一个总的损失函数，即 $L$，是由训练数据带来的损失结合一个正则项得到的，正则项表示我们模型的复杂度，为了更好地泛化，我们倾向于取简单的模型。现在我们想要找到与最小损失对应的参数 $W$。我们想要最小化损失函数，为了做到这一点，我们要找到 $L$ 在 $W​$ 方向上的梯度。上节课我们讲了如何使用最优化做到这一点，我们会沿着最陡的下降方向，即梯度的负方向，来一步步迭代，这样就能沿着损失函数从上往下走到最低点。我们可以看到梯度下降基本上沿着这个轨迹，走到了损失函数的等高轮廓线的最低点。我们还讨论了求梯度的不同方法。我们可以使用有限差分估计来计算数值梯度，这个方法用起来很慢而且结果区分度不大，但是写起来简单，你总是可以通过这种方式得到梯度。我们还讲了如何使用解析梯度计算，这个方法很快并且得到的解析梯度的表达式是精确的，但是同时你需要数学和微积分知识来推到这些结果，所以容易弄错。所以在实践中，我们想要推导和使用解析梯度，但是同时用数值梯度来检查我们的实现结果以确保我们的推导是正确的。

 We have talked about how to define a classifier using a function $f​$, parameterized by weights $W​$, and this function $f​$ parametrized by weights $W​$, and this function f is going to take data x as input, and output a vector of scores for each of the classes that you want to classify. And from here we can also define a loss function, so for example, the SVM loss function that we talked about which basically quantifies how happy or unhappy we are with the scores that we've produced. And we can use that to define a total loss term, so $L​$ here, which is a combination of this data term and a regularization term that expresses how simple our model is, and we have a preference for simpler models, for better generalization. And so now, we want to find the parameters $W​$. We want to minimize the loss function, and to do that we want to find the gradient of $L​$ with respect to $W​$. Last lecture we talked about how we can do this using optimization, and we're going to iteratively take steps in the direction of deepest descent, which is the negative of the gradient, in order to walk down this loss landscape and get the point of lowest loss. And we saw how this gradient descent can basically take this trajectory, looking like this image on the right, getting to the bottom of your loss landscape. And we also talked about different ways for computing a gradient. We can compute this numerically using finite difference approximation which is slow and approximate, but at the same time it's really easy to write out, you can always get the gradient this way. We also talked about how to use the analytic gradient and computing, it's fast and exact once you gotten the expression for the analytic gradient, but at the same time you have to do all the math and the calculus to derive this, so is's also easy to make mistakes. So in practice, what we want to do is we want to derive the analytic gradient and use this,  but at the same time check our implementation using the numerical gradient to make sure that we've gotten all of our math right.

------

<center>
    <image src = "./images/4.1_02_computational-graph.png"></image>
</center>
------

我们今天要讲如何计算任意复杂函数的解析梯度要用到一个叫做**计算图**的框架。大体上说，计算图就是我们用这类图来表示任意函数，其中图的节点表示我们要执行的每一步计算。我们讲过的线性分类器，输入是 $x$ 和 $W$，这个乘的节点表示矩阵乘法，是数据 $x$ 和参数 $W$ 的乘积，输出得分向量。接下来是另一个计算节点，表示 hinge loss，计算数据损失项 $L{}{_i}$。右下角还有一个正则项，这个节点计算了正则项，在最后的总的损失 $L​$ 是正则项和数据项的和。这里的好处是，一旦我们能用计算图来表示一个函数，那就能用所谓的**反向传播技术**递归地调用链式法则来算计算图中每个变量的梯度，我们将会看到这是怎么做到的。

Today we're going to talk about how to compute the analytic gradient for arbitrarily complex functions using a framework that I'm going to call **computational graphs**. Basically what a computational graph is that we can use this kind of graph in order to present any function, where the nodes of the graph are steps of computation that we go through. In this example the linear classifier that we've talked about, the inputs here are $x$ and $W$, and this multiplication node represents the matrix multiplier, the multiplication of the parameters $W$ with our data $x$ that we have, outputting our vector of scores. And then we have another computational node which represents our hinge loss, $L{}{_i}$. And we also have this regularization term at the bottom right, so this node which computes our regularization term, and then our total loss here at the end, $L$, is the sum of the regularization term and the data term. And the advantage here is that once we can express a function using a computational graph, then we can use a technique that we call **backpropagation** which is going to recursively use the chain rule in order to compute the gradient with respect to every variable in the computational graph, and we're going to see how this is done.

当我们开始涉及非常复杂的函数时，这些方法非常有用，比如这门课后面要讲到的卷积神经网络。这里最上面是输入的图片，最下面是损失，输入必须要穿过中间的很多转换层才能最终得到最下面的损失函数。这个时候会很夸张，比如神经图灵机，这是另一种深度学习模型，这个例子中的计算图特别夸张，这里沿着时间展开的话，如果你要计算任意这些中间变量的梯度，几乎是不切实际的。

And this becomes very useful when we start working with really complex functions, for example, convolutional neural networks that we're going to talk about later in this class. We have here the input image at the top, we have our loss at the bottom, and the input has to go through many layers of transformations in order to get all the way down to the loss function. And this can get even crazier with things like a neural Turing machine, which is another kind of deep learning model, and in this case you can see that the computational graph for this is really insane, and especially, we end up, unrolling this over time. It's basically completely impractical if you want to compute the gradients for any of these intermediate variables.

## 4.1.2 一维反向传播 One-dimensional Backpropagation

那么反向传播是如何工作的呢？我们从一个简单的例子讲起，假设有一个函数，$f = (x + y) * z$，我们要找到函数输出对应任意变量的梯度。第一步是用计算图来表示我们的整个函数。计算图如右边所示，首先是“加”节点，即 $x+y$，然后是“乘”节点。现在要做这个网络的前向传播，给定了每个变量对应的值，$x = -1, y = 5, z = -4$。把这些值写到计算图中，然后计算得到中间值， $x + y = 3​$，然后通过最后一个节点，乘法运算，得到最后一个节点的值是 -12。

So how does backpropagation work? We're going to start off with a simple example, where again, our goal is that we have a function, in this case, $f = (x + y) * z$, and we want to find the gradients of the output of  the function with respect to any of the variables. So the first step is we want to take our function f, and we want to represent it using a computational graph. So here our computational graph is on the right, and you can see that first we have the plus node, so $x+y$, and then we have this multiplication node for the second computation that we're doing. And now we're going to do a forward pass of this network, so given the values of the variables that we have, $x = -1, y = 5, z = -4$. I'm going to fill these all in our computational graph, and the we can compute an intermediate value, $x + y = 3$, and then finally we pass it through again, through the last node, the multiplication, to get our final node of f equals negative 12.

------

<center>
    <image src = "./images/4.1_03_chain-rule.png"></image>
</center>
------

这里我们想给每个中间变量一个名字。加法节点叫做 $q = x + y$，然后是节点 $f = q * z$，用到了中间节点 $q$。我把梯度也写在这里了，因为是加法，$q$ 对应 $x$ 和 $y$ 的梯度都是 1，因为是乘法，$f$ 在 $q$ 和 $z$ 方向上的梯度分别是 $z$ 和 $q​$。**反向传播是链式法则的递归调用，我们从计算图的最后面开始，从后往前走，计算所有的梯度。**

We want to give every intermediate variable a name. We have $q = x + y​$, and then $f = q * z​$, using this intermediate node. And I've also written out here, the gradients of $q​$ with respect to $x​$ and $y​$, which are just one, because of the addition, and then the gradients of $f​$ with respect to $q​$ and $z​$, which is $z​$ and $q​$ respectively because of the multiplication rule. So the backprop is a recursive application of the chain rule, so we're going to start at the very end of the computational graph, and we're going to work our way backwards and compute all the gradients along the way.

我们可以精确写出结果，只用微积分找到所有的结果。比如，我们要计算 $df/dx$，我们可能要扩展这个表达式，可以看到结果会是 $z​$。但是在这种情况下我们可以做到这一点，因为函数简单，但我们会在后面看到一些例子，一旦表达式变得非常复杂，你不会想用微积分去直接计算某些变量的梯度，对于一个超级复杂的表达式来说，如果你使用这种形式，可以把复杂的表达式分解成一些计算节点，然后就可以仅仅用非常简单的计算就可以得出梯度，加法、乘法、指数这种程度的简单计算，然后不用推导整个表达式，你只是用链式法则把他们乘到一起就能得到梯度值。

We could exactly write out all of these using just calculus. You know, we want $df/dx​$, and we can probably expand out this expression and see it's just going to be $z​$. But we can do this in this case because it's simple, but we'll see examples later on where once this becomes a really complicated expression, you don't want to have to use calculus to derive the gradient for something, for super complicated expression and instead, if you use this formalism and you break it down into these computational nodes, and you can only ever work with gradients of very simple computations, at the level of additions, multiplications, exponentials, things as simple as you want them, and you just use the chain rule to multiply all these together and get value of gradient without having to ever derive the entire expression.

所以在反向传播算法中接下来做的是在计算图中基本上有所有的这些节点，每个节点只知道与它直接相连接的节点。所以在每个节点上，我们有连接这个节点的本地输入，也就是流入这个节点的值，我们也有这个节点直接的输出。这里我们本地输入的是 $x$ 和 $y $ 输出的是 $z$。我们也知道这个节点的梯度，我们可以计算出关于 $z$ 在 $x$ 方向上的梯度 $(dz/dx)$，和 $z$ 在 $y$ 方向上的梯度 $(dz/dy)$，这些通常是很简单的运算。每个节点将会像之前例子里的加法和乘法一样，我们可以写出梯度，我们不需要使用非常复杂的微积分就可以计算出结果。

So what we're doing is, in backprop, is we basically have all of these nodes in our computational graph, but each node is only aware of its immediate surroudings. So at each node, we have the local inputs that are connected to this node, the values that are flowing into the node, and we also have the output that is directly outputted from this node. Here our local inputs are  $x$ and $y $, and the output is $z$. And at this node we also know the local gradient, we can compute the gradient $(dz/dx)$,  and the gradient $(dz/dy)$, and these are usually really simple operations. Each node is going to be something like the addition or the multiplication that we had in that earlier example, which is something where we can just write down the gradient, and we don't have to, go through very complex calculus in order to find this. 

------

<center>
    <image src = "./images/4.1_04_chain-rule2.png"></image>
</center>
------

我们有这些节点，每个节点基本上都能得到它的本地输入和直接传递到下一个节点的输出，相对于节点的输入，我们也可以计算出它们对应的梯度是节点的直接输出。在反向传播中发生了什么，我们从图的后面开始，然后使用我们的方法从最后的地方一直到最开始的地方，当我们到达每一个节点时，每一个节点都会得到一个从上游返回的梯度，这个梯度是对这个节点的输出的求导。等到我们在反向传播中到达这个节点，我们就计算出了最终损失 $L​$ 关于 $z​$ 的梯度。接下来想做的是找到关于节点输入的梯度，即在 $x​$ 和 $y​$ 方向上的梯度。就像我们之前看到的，用链式法则去计算损失函数关于 $x​$ 的梯度就是 $L​$ 在 $z​$ 方向上的梯度乘以 $z​$ 在 $x​$ 方向上的本地梯度。在链式法则中，我们通常把上游的梯度值传回来，再乘以本地梯度值，从而得到关于输入的梯度值。

We have these nodes and each node basically gets its local inputs coming in and the output that it sees directly passing on to the next node, and we also have these local gradients that we computed, the gradient of the immediate output of the node with respect to the inputs coming in. So what happens during backprop is we'll start form the back of the graph, and we work our way from the end all the way back to the beginning, and when we reach each node, at each node we have the upstream gradients coming back, with respect to the immediate output of the node. So by the time we reach this node in backprop, we've already computed the gradient of our final loss $L$, with respect to $z$. And now what we want to find next is the gradients with respect to just before the node, to the values of $x$ and $y$. And so as we saw earlier, we do this using the chain rule,  the gradient of this loss function with respect to $x$ is going to be the gradient with respect to $z$ times, compounded by this local gradient of $z$ with respect to $x$. So in the chain rule we always take this upstream gradient coming down, and we multiply by the local gradient in order to get the gradient with respect to the input.

------
<center>
    <image src = "./images/4.1_05_chain-rule3.png"></image>
</center>
------

接下来，我们来看另一个例子，这个例子要复杂一点，这个例子也证明了反向传播为什么这么有效。在这个例子里，$f​$ 是关于 $w​$ 和 $x​$ 的函数，它等于 $\frac {1}{1+e{}{^{-(w{}{_0}x{}{_0}+w{}{_1}x{}{_1}+w{}{_2})}}}​$。通常我们的第一步是把它转换成一个计算图，在这个计算图中，我们可以看出，我们将 $w​$ 和 $x​$ 项相乘，也就是 $w{}{_0}​$ 乘以  $x{}{_0}​$，$w{}{_1}​$ 乘以  $x{}{_1}​$ 以及  $w{}{_2}​$，我们将三项加起来，然后取负， 再求关于它的指数，再加 1，最后取倒数。我在图上写下了一些值，有了初始值，我们就可以进行前向传播，基本上就可以计算出每个阶段的结果。在底部写出了一些微分表达式，这些在后面非常有用。

We're going to go through another example, this time a little bit more complex, so we can see more why backprop is so useful. In this case, our function is $f$ of $w$ and $x$, which is  $\frac {1}{1+e{}{^{-(w{}{_0}x{}{_0}+w{}{_1}x{}{_1}+w{}{_2})}}}$. So again, the first step always is we want to write this out as a computational graph, in this case, we can see that in this graph, first we multiply together the w and x terms that we have, $w_0$ by $x_0$, $w_1$ by $x_1$, and $w_2$, then we add all of these together, then scale it by negative one, we take the exponential, we add one, and we finally we do one over this whole term. And here I've also filled in values of these, given values that we have, we can make a forward pass and basically compute what the value is at every stage of the computation. And here I've also written down at the bottom the values, the expressions for some derivatives that are going to be helpful later on, so same as we did before with the simple example.

------

<center>
    <image src = "./images/4.1_06_chain-rule4.png"></image>
</center>
------

我们要对它进行反向传播，我们从最末端开始。输出最终变量在某方向上的梯度结果得到 1，这没什么说的。然后反向进行下一步，那么 $1/x$ 关于输入值的导数是什么呢？在本例中，我们知道传回的上游梯度值就在图上的红框中，这就是传回来的上游梯度，现在我们需要找到本地梯度，关于这个节点 $1/x$ 的本地梯度 $df/dx$ 是 $-1/x{}{^2}$，带入我们在前向传播中得到的 $x$ 的值为 1.37，最后关于这个变量的梯度就是  $-1/1.37{}{^2}$，等于 -0.53。

So now then we're going to do backprop through here, we'll start at the very end of the graph. The gradient of the output with respect to the last variable is just one, it's just trivial, and so now moving backwards one step, what's the gradient with respect to the input just before one over x? In this case, we know that the upstream gradient that we have coming down, is this red one. This is the upstream gradient that we have flowing down, and now we need to find the local gradient, and the local gradient $df/dx$ of this node $1/x$ is $-1/x{}{^2}$. So here we're going to take $-1/x{}{^2}$ and plug in the value of x that we had during this forward pass, 1.37, and so our final gradient with respect to this variable is going to be  $-1/1.37{}{^2}$ equals negative 0.53.

然后到下一个节点，我们用同样的流程。我们知道从上游传回来的梯度是 -0.53，因为节点是加 1，我们参照图下面的微分表达式，我们有一个常数加 $x$ 的微分表达式，本地梯度就是 1 了。那么使用链式法则时，对这个变量怎么求梯度呢？这就要用上游梯度值 -0.53 乘以本地梯度值 1，结果等于 -0.53。

So moving back to the next node, we're going to go through the exact same process. So here, the gradient flowing from upstream is going to be negative 0.53, and here the local gradient, the node here is a plus one,  and now looking at our reference of derivatives at the bottom, we have that for a constant plus $x$, the local gradient is just one. So what's the gradient with respect to this variable using the chain rule? So it's going to be the upstream gradient of -0.53 times our local gradient of one, which is equal to -0.53.

------

<center>
    <image src = "./images/4.1_07_chain-rule5.png"></image>
</center>
------

我们再次进行反向传播的下一步，这里有一个指数，上游梯度传下来的是什么呢？上游梯度是 -0.53，本地梯度呢？就是指数 $e$ 对 $x$ 的求导。这是一个指数节点，链式法则告诉我们梯度等于 -0.53 乘以 $e$ 的 $x$ 次方，例子中$x$ 的值通过前向传播得到是 -1，最终梯度是 -0.2。

So let's keep moving backwards one more step, here we have the exponential, so what's the upstream gradient coming down? So the upstream gradient is -0.53, what's the local gradient here? It's going to be the local gradient of $e$ to the $x$. This is an exponential node, and our chain rule tells us that gradient is -0.53 times $e$ to the power of $x$, which in this case is negative one, from our forward pass, and this is going to give us our final gradient of -0.2.

------

<center>
    <image src = "./images/4.1_08_chain-rule6.png"></image>
</center>
------

下一个节点，这个节点是乘以 -1，从上游传回来的是 -0.2，本地梯度是 -1，所以这里的梯度是 -1 乘以 -0.2 等于 0.2。

One more node here, this node is a multiplication with negative one, what's the upstream gradient coming down is -0.2, and local gradient is -1, so we have the gradient here is -1 times -0.2 equals 0.2.

------
<center>
    <image src = "./images/4.1_09_chain-rule7.png"></image>
</center>
<center>
    <image src = "./images/4.1_10_chain-rule8.png"></image>
</center>
------

这里我们已经完成了大多数的梯度的计算，之前有个问题相比于推导对任意参数的梯度的解析表达式，为什么这样计算会简单一些，从这里可以看出，所有我们处理过的本地梯度的表达式，我们先写出这些表达式，所以一旦有了这些本地梯度表达式，我们需要做的是填充每个值，然后使用链式法则从后往前乘以这些值，得到对所有变量的梯度。

So here we've filled out most of these gradients, and there was the question earlier about why this is simpler than just computing, deriving the analytic gradient, the expression with respect to any of these variables. And so you can see here, all we ever dealt with was expression for local gradients that we had to write out, so once we had these expressions for local gradients, all we did was plug in the values for each of these that we have, and use the chain rule to numerically multiply this all the way backwards and get the gradients with respect to all of the variables.

因此这里，我们也可以用相同的方法得到在 $w{}{_1}​$ 和 $x{}{_1}​$ 方向上的梯度，我想提醒的是当我们创建这些计算图的时候，我们可以以我们想要的任意间隔尺寸来定义计算节点。在这种情况下我们把它分解成我们能够达到的最简单形式，分解成加法和乘法运算，基本上没有比这更简单的了，但实际上，如果需要的话，我们可以把一些节点组合在一起形成更复杂的节点。只要我们能写出那个节点的本地梯度。举个例子，比如这个 sigmoid 函数，我已经定义了 sigmoid 函数见右上角，$\frac {1}{1+e{}{^{-x}}}​$，这是一个十分常见的函数，在本课程接下来的时间里，大家会经常见到它，我们可以计算它的梯度，并写出这个梯度的表达式，如果我们用数学知识推导出它的解析式，我们最终可以得到一个优美的表达式。在这里，它等于 $\frac {e{}{^{-x}}} {(1+e{}{^{-x}}){}{^2}}​$ 即 $\frac {1+e^{-x}-1}{1+e^{-x}}\frac {1}{1+e^{-x}}​$，即 $(1-\sigma(x))\sigma(x)​$。类似于这种情况，我么可以得到计算图中构成这个 sigmoid 函数的计算节点，然后用一个大的 sigmoid 节点替换掉。因为我们知道这个门的本地梯度，就是这个表达式，$d\sigma(x)/dx​$。这里最重要的事情是你能聚合你想要的任意节点去组成一些在某种程度上稍微复杂的节点，只要你能写出它的本地梯度。这就是一个平衡，一方面是你想用多少数学计算，去得到一个更加简洁和简单的计算图，另一方面是你想要每个梯度有多简单。然后你就可以按照你想要的复杂度，写出一个计算图。

And so, we can also fill out the gradients with respect to $w_1$ and $x_1$ here in exactly the same way, and so one thing that I want to note is that when we're creating these computational graphs, we can define the computational nodes at any granularity that we want to. So in this case, we broke it down into the absolute simplest that we could, down into additions and multiplications, it basically can't get any simpler than that, but in practice, we can group some of these nodes together into more complex nodes if we want. As long as we are able to write down the local gradient for the node.  So as an example, if we look at a sigmoid function, I've defined the sigmoid function in the upper-right here, $\frac {1}{1+e{}{^{-x}}}$, and this is a really common function, you'll see a lot in the rest of this class, and we can compute the gradient for this, we can write it out, and if we do actually go through the math of doing this analytically, we can get a nice expression at the end. So in this case, it's equal to one minus sigma of x, so the output of this function $\frac {e{}{^{-x}}} {(1+e{}{^{-x}}){}{^2}}$ , $(1-\sigma(x))\sigma(x)$. And in cases where we have something like this, we could just take all the computations that we had in our graph that made up this sigmoid, and we could just replace it with one big node that's sigmoid, because we do know this local gradient for this gate, it's this expression, $d\sigma(x)/dx$. The import thing here is that you can group any nodes that you want to make any sorts of a little bit more complex nodes, as long as you can write down the local gradient for this. And all this is basically a trade-off between, how much math you want to do in order to get a more, kind of concise and simpler graph, versus how simple you want each of your gradients to be. And you can write out as complex of a computational graph that you want.

------
<center>
    <image src = "./images/4.1_11_chain-rule9.png"></image>
</center>
------

我真的很喜欢思考计算图这个东西，这让我感觉很开心。任何时候，我必须得到一个梯度，即使这些我想要计算的梯度的表达式非常繁琐和吓人，无论是像 sigmoid 函数还是其他更复杂的函数，如果我想做的话，我可以推导出这个，但实际上，如果我只是坐下来并写出求解这样一个计算图，我可以很简单地算出来，只要我能够应用反向传播和链式法则，我就能计算所需要的所有梯度。我们介绍了如何把这些节点组合在一起，变成一个 sigmoid 门，并且确认这是等价的，我们可以把数值带入到表达式。这里对 sigmoid 门的输入，是绿色的 1，然后得到输出为 0.73，如果把数值带入到 sigmoid 函数这个就能求解出来。现在，如果我们想要得到这个梯度，并且我们想要这整个 sigmoid 函数，作为一个节点，我们应该做的是我们需要使用已经推导出来的本地梯度，$(1-\sigma(x))\sigma(x)​$。因此如果我们把数值带入到表达式，我们已经知道 sigmoid(x) 的值是 0.73，我们会看到这个梯度的取值等于 0.2，所以这个本地梯度的值也是 0.2。我们把它乘上上游梯度，也就是 1，我们将会在 sigmoid 门的最前端得到相同的梯度值，就像我们把它分解成所有这些更小的计算节点。

I really like thinking about computational graph and I feel very comforted. Like anytime I have to take a gradient, find gradients of something, even if the expression that I want to compute gradients of is really hairy, and really scary, whether it's something like sigmoid or something worse, I know that, I could derive this if I want to, but really, if I just sit down and write it out in terms of a computational graph, I can go as simple as I need to always be able to apply backprop and the chain rule, and be able to compute all the gradients that I need. We talked about how we could group these set of nodes together into a sigmoid gate, and just to confirm that this is actually exactly equivalent, we can plug this in. We have that our input here to the sigmoid gate is going to be 1, in green, and then we have that the output is going to be here, 0.73, and this will work out if you plug it into the sigmoid function. And now if we want to take the gradient, and we want to treat this entire sigmoid as one node, now what we should do is we need to use this local gradient that we've derived up here, $(1-\sigma(x))\sigma(x)$. So if we plug this in, and here we know that the value of sigmoid(x) was 0.73, we'll see that the value of this gradient is equal to 0.2, so the local gradient value is 0.2, we multiply it by the x upstream gradient which is one, and we'll get out exactly the same value of the gradient with respect to before the sigmoid gate, as if we broke it down into all of the smaller computations.

------
<center>
    <image src = "./images/4.1_12_pattern-in-backflow.png"></image>
</center>
------

我们看看当我们通过计算图，反向传播这些梯度时，你会注意到有一些模式，并且其中一些模式我们能给出直观的解释。我们看到**加法门是一个梯度分发器**，当我们通过加法门连接了两个分支，它获取了上游梯度，并且只是分发和传递完全相同的梯度给相连接的两个分支。这里我们可以思考一些更多的东西。max 门会是怎样的？在下面有一个 max 门，它的输入是 $z$ 和 $w$，$z$ 取值为 2，$w$ 取值为 -1。然后我们取其中的最大值，也就是 2，我们将这个值传递到剩余的计算图中。现在，如果我们得到对 max 门的梯度，也就是上游梯度，假设是 2，这个本地梯度会是怎样？$z$ 的梯度会是 2，$w$ 的梯度会是 0。所以其中一个变量会得到刚传递回来的梯度完整值，并且再传递给那个变量，然后另一个变量的梯度会取为 0。所以我们可以把这个想象成一个梯度路由器。加法节点回传相同的梯度给进来的两个分支，**max 门将获取梯度并且将其路由**到它其中一个分支。这样是说得通的，因为我们查看前向传递可以看到，只有最大值能被传递到计算图的剩余部分。所以只有这个最大值能真正影响到我们函数计算的唯一值。因此这样是说得通的，当我们传递梯度回去时，我们只想要通过这个分支来调整它。那么**乘法门**是怎样的呢？本地梯度是另一个变量的值。所以我们可以把它认作是一个**梯度转换器，尺度缩放器**，我们获取上游梯度，然后根据另一个分支的值对其缩放。

We're looking at what's happening as we're taking these gradients going backwards through our computational graph, there's some patterns that you'll notice where there's some intuitive interpretation that we can give these. We saw the add gate is a gradient distributor, when we passed through this addition gate here which had two branches coming out of it, it took the gradient, the upstream gradient and it just distributed it, passed the exact same thing to both of the branches that were connected. Here's a couple more that we can think about. What's the max gate look like? So we have a max gate here at the bottom, where the input's coming in are $z$ and $w$, $z$ has a value of two, $w$ has a value of -1. And we took the max of this, which is 2. We pass this down into the remainder of our computational graph. So now, if we take the gradients with respect to this, the upstream gradient, let's say two coming back, and what does this local gradient look like? z will have a gradient of 2, w will have a gradient of 0. So one of these is going to get the full value of the gradient just passed back, and routed to that variable, and then the other one will have a gradient of zero, and so we can think of this as kind of a gradient router. Whereas the addition node passed back the same gradient to both branches coming in, the max gate will just take the gradient and route it to one of the branches. And this makes sense because if we look at our forward pass, what's happening is that only the value that was the maximum got passed down to the rest of the computational graph. So it's the only value that actually affected our function computation at the end. And it makes sense that when we pass our gradients back, we just want to flow it through that branch of the computation. And so what's a multiplication gate? The local gradient is just the value of the other variable. So we can just think of this as a gradient switcher. A switcher, and I guess a scaler, where we take the upstream gradient and we scale it by the value of the other branch.

------
<center>
    <image src = "./images/4.1_13_add-gradient-at-branch.png"></image>
</center>
------

 另一个需要说明的事情是当我们有一个节点连接到多个节点时，梯度会在这个节点累加。在这些分支上，根据多元链式法则，我们只会获取这些每个节点返回的上游梯度值。并且，我们会将他们加起来，得到回流到这个节点的总上游梯度。你可以从多元链式法则中了解到这个。你可以这样思考这个问题，如果要改变这个节点一点点，当你正通过这个图进行前向传递时，它会在前向传递中影响到所有连接这个节点的节点。当你进行反向传播时，所有传回的梯度都会影响这个节点。我们将这些加起来得到回流到这个节点的总上游梯度。

And one another thing to note is that when we have a place where one node is connected to multiple nodes, the gradients add up at this node. So at these branches, using the multivariate chain rule we're just going to take the value of the upstream gradient coming back from each of these nodes, and we'll add these together to get the total upstream gradient that's flowing back into this node, and you can see this from the multivariate chain rule and also thinking about this, if you're going to change this node a little bit, it's going to affect both of these connected nodes in the forward pass, when you're making your forward pass through the graph. And when you're doing backprop, both of these gradients coming back are going to affect this node, and so that's how we'll sum these up to be the total upstream gradient flowing back into this node.

我们没有做任何操作来更新这些权重的值，我们只是发现了对这些变量的梯度。我们在本次课程中谈到的是如何在我们的函数中计算对于任何变量的梯度，然后，一旦我们得到了这些梯度，我们就可以将刚学到的知识应用到上一节关于优化的课里面。在给定梯度的情况下，我们沿着梯度方向前进一步，就可以更新我们的权重，也就是参数。所以我们上节课学到的东西是最优化问题的整体框架，而今天学到的是如何计算任意复杂函数的梯度。当我们后续讨论到神经网络中的复杂函数时，这是非常有用的。

We've not done anything yet to update the values of these weights, we've only found the gradients with respect to the variables, that's exactly. So what we've talked about so far in this lecture is how to compute gradients with respect to any variables in our function, and then once we have these, we can just apply everything we learned in the optimization lecture. So given the gradient, we now take a step in the direction of the gradient in order to update our weight, our parameters. So you can just take this entire framework that we learned about last lecture for optimization, and what we've done here is just learn how to compute the gradients we need for arbitrarily complex functions, and so this is going to be useful when we talk about complex functions like neural networks later on.

------
<center>
    <image src = "./images/4.1_14_gradient-for-vectors.png"></image>
</center>
------

## 4.1.3 高维反向传播 High-dimensional Backpropagation

讲完了这些一维的例子，我们要看一看变量是高维向量的情况。假设我们有 $x$ $y$ $z$ 三个变量，三个向量而不是标量。这时整个计算流程还是一样的，唯一的区别在于我们刚才的梯度变成了雅克比矩阵，所以现在是一个包含了每个变量里各元素导数的矩阵，比如 $z$ 在每个 $x$ 元素方向上的梯度。看这样一个例子，我们有一个 4096 维的向量作为输入，在接下来要学的卷积神经网络中，这样的数据尺寸是比较常见的，中间的运算节点是对每个元素求最大值的运算。我们要求的 $f$ 是 $x​$ 中每个元素和零之间的较大值，输出也是一个 4096 维的向量。在这个例子里，这个雅克比矩阵的尺寸是多大？雅克比矩阵每一行都是偏导数，矩阵的每个元素是输出向量的每个元素对输入向量每个元素分别求偏导的结果。所以矩阵是 4096 乘 4096 的。实际应用中可能会遇到更大的雅克比矩阵，因为我们要进行批处理，比如一次同时有输入 100 个变量，我们把这些一起送进运算节点以提高效率，那么这个矩阵长宽都要再乘以 100，这个雅克比矩阵尺寸就会 409600 乘以 409600。这个数据量太大，而且完全无法实现。

Now that we've done all these examples in the scalar case, we're going to look at what happens when we have vectors.  So now if we variables $x$, $y$ and $z$, instead of just being numbers, we have vectors for these. Everything stays exactly the same, the entire flow, the only difference is that our gradients are going to be Jacobian matrices. So these are now going to be matrices containing the derivative of each element of, for example $z$ with respect to each element of $x$. Give an example of something where this is happening, let's say that we have our input is going to now be a vector, 4096-dimensional input vector, and this is kind of a common size that you might see in convolutional neural networks later on, and our node is going to be an element-wise maximum, so we have $f$ of $x$ is equal to the maximum of $x$ compared with zero element-wise, and then our output is going to be also a 4096-dimensional vector. In this case, what's the size of Jacobian matrix? Remember I said earlier, the Jacobian matrix is going to be, like each row is going to be partial derivatives, a matrix of partial derivatives of each dimension of the output with respect to each dimension of the input. So the matrix is 4096 squared. In practice this is going to be even larger because we're going to work with many batches, for example, 100 inputs at the same time, and we'll put all of these through our nodes at the same time to be more efficient, and so this is going to scale this by 100, and practice our Jacobian's actually going to turn out to be something like 409,600 by 409,600. This is really huge, and basically completely impractical to work with. 

------
<center>
    <image src = "./images/4.1_15_vectorized-operations.png"></image>
</center>
------

实际应用中，大多数情况下不需要计算这么大的雅克比矩阵，为什么是这样呢，这就要看看这个雅可比矩阵有什么特点。思考一下这里的计算，我们对每个元素求最大值，再想一想对每一个偏导计算的结果，输入的某个元素和输出的某个元素之间有什么对应关系。我们能在这个雅可比矩阵中找到什么特点呢？对角矩阵。因为是对每个元素分别运算，比如输入里的第一个元素只和输出里的第一个元素有联系，所以算出的雅克比矩阵就会是一个对角矩阵。所以在实际应用中，我们并不需要把整个矩阵全部写出来，我们只需要求输出向量关于 $x​$ 的偏导，然后把结果作为梯度填进去。

In practice, we don't really need to compute this huge Jacobian most of the time, and why is it, what does this Jacobian matrix look like. If we think about what's happening here, where we're taking this element-wise maximum, and we think about what are each of the partial derivatives, which dimensions of the inputs affect which dimensions of the output? What sort of structure can we see in our Jacobian matrix? It's diagonal. Because this is element-wise, each element of the input, say the first dimension, only affects that corresponding element in the output. And because of that our Jacobian matrix, which is just going to be a diagonal matrix. And so in practice, we don't actually have to write out and formulate this entire Jacobian, we can just know the effect of $x​$ on the output. And then we can just use these values and fill it in as we're computing the gradient.

------
<center>
    <image src = "./images/4.1_16_vectorized-example1.png"></image>
</center>
------

现在我们看看更具体的输入是向量的例子。在这个例子中，我们有关于 $x$ 和 $W$ 的函数 $f$，等于 $W$ 与 $x$ 乘积的 $L2$ 范数，设 $x$ 是一个 $n$ 维点的向量，$W$ 是一个 $n * n$ 的矩阵。和之前一样，第一步，把计算图写出来。$W$ 与 $x$ 相乘，然后进行 $L2$ 范数计算。我们给变量赋值，设 $W$ 是这样一个 2*2 矩阵，$x$ 是二维向量。再看一下中间结果，经过乘积后的中间结果记作 $q$，我们有 $q = Wx$，写成元素相乘的形式，第一个元素是 $W{}{_{11}}x{}{_1}+\dots+W{}{_{1n}}x{}{_n}$，然后我们可以用 $q$ 表示出 $f$。观察第二个节点，我们有 $f(q) = ||q||{}{^2}$，即 $q{}{^2_1}+q{}{^2_2}+\dots+q{}{^2_n}$ 。把结果写进去，我们求出了 $q$ 和最后的输出。现在我们开始计算反向传播，还是，第一步，输出梯度是 1，现在反向前进一个节点，我们想求关于 $L2$ 范数前的变量 $q$ 方向上的梯度。$q$ 是一个二维向量，我们想做的事情是找到 $q$ 的每个元素是怎样影响 $f$ 最终的值的。如果我们观察一下公式，页面底部的 $f$ 的表达式，我们可以发现 $f$ 在特定方向 $q{}{_i}$ 上的梯度，比如 $q{}{_1}$ ，正好是 2 倍的 $q{}{_i}$。这里做了求导，我们有关于每个 $q{}{_i}$ 的式子，我们也可以写出它的向量形式，它就是 2 倍的 $q$ 向量。如果我们写成向量形式，接着我们得到梯度是 0.44 和 0.52 的向量。可以看到，它只是用了 $q$ 并扩大了 2 倍，每个元素都乘以 2。所以向量的梯度总是和原向量保持相同大小，每个梯度的元素代表这个特定元素对最终该函数影响的大小。

Now we're going to go through a more concrete vectorized example of a computational graph. Let's look at a case where we have the function f of $x$ and $W$ is equal to the $L2$ of $W$ multiplied by $x$. $x$ is a n-dimensional vector and $W$ is $n * n$. So again, our first step is writing out the computational graph. We have $W$ multiplied by $x$, and then followed by, I'm just going to call this $L2$. And so now let's also fill out some values for this, let's say have $W$ be this two by two matrix, and $x$ is two-dimensional vector. And label again our intermediate nodes, after the multiplication, we have $q$ equals $W$ times $x$ which we can write out element-wise this way, where the first element is just  $W{}{_{11}}x{}{_1}+\dots+W{}{_{1n}}x{}{_n}$, and then we can express f in relation to $q$. Looking at the second node we have $f(q) = ||q||{}{^2}$, which is equal to $q{}{^2_1}+q{}{^2_2}+\dots+q{}{^2_n}$. So we filled this in, we get $q$ and then we get our final output. So now let's do backprop through this, again, this is always the first step, we have the gradient with respect to our output is just one. Now let's move back one node, so now we want to find the gradient with respect to q, our intermediate variable before the $L2$. And so $q$ is a 2-dimensional vector, and what we want to do is we want to find how each element of $q$ affects our final value of $f$, and if we look at this expression that we've written out for f here at the bottom, we can see that the gradient of $f$ with respect to a specific $q{}{_i}$, let's say  $q{}{_1}$, is just going to be 2 times  $q{}{_i}$. This is just taking this derivative here, and we have this expression with respect to each element of $q{}{_i}$, we could also write this out in vector form if we want to, it's just going to be 2 times our vector of $q$. if we want to write this out in vector form, and so what we get is that our gradient is 0.44 and 0.52, this vector. And so you can see that it just took $q​$, and it scaled it by 2. Each element is just multiplied by 2. So the gradient of a vector is always going to be the same size as the original vector, and each element of this gradient means how much of this particular element affects our final output of the function.

------
<center>
    <image src = "./images/4.1_17_vectorized-example2.png"></image>
</center>
------

现在我们倒退一步讲，关于 $W$ 的梯度是什么？这里我们用相同的概念应用链式法则。我们希望计算关于 $W$ 的 $q$ 的本地梯度，如果我们这么做，我们观察一下对每个 $q$ 的影响，$q$ 的每个元素对应 $W$ 的每个元素，这就是之前说过的雅克比矩阵。在这乘法公式中 $q = Wx$，什么是导数或者说 $q$ 的第一个元素的梯度。我们的第一个元素对应 $W{}{_{11}}$，所以 $q{}{_{1}}$ 对应 $W{}{_{11}}$ 么？$x{}{_{1}}$，我们知道结果是 $x{}{_{1}}$，我们可以把这个推广到一般关于 $W{}{_{ij}}$ 的 $q{}{_{k}}$ 的梯度等于 $x{}{_{j}}$。现在如果我们想找到对应的梯度，关于每个 $W{}{_{ij}}$ 的 $f$ 的梯度。可以直接看这些导数，我们可以用之前讨论过的链式法则，我们可以通过 $dq{}{_k}$ 合并 $df$，对应每个 $W{}{_{ij}}$ 元素对应 $dq{}{_k}$ 的 $q$ 的每个元素，我们得到了每个 $W$ 的元素对每个 $q$ 的影响，对所有的 $q$ 求和。如果我们写出来，公式是 $2q{}{_i}x{}{_j}$. 计算可以得到关于 $W​$ 的梯度。我们按照元素级别计算，看一下我们推导出的公式，写成向量形式。记住一个很重要的事情，检查变量梯度的向量大小，应该和变量向量大小一致，这在实际应用中时非常有用的完整性检查，一旦计算出梯度之后，检查梯度向量的大小，因为每个梯度元素量化了每个元素对最终输出影响的贡献。

So now let's move one step backwards, what's the gradient with respect to $W​$? And so here again we want to use the same concept of trying to apply the chain rule, so we want to compute our local gradient of $q​$ with respect to $W​$, and so let's look at this again element-wise, and if we do that, let's see what's the effect of each $q​$, each element of $q​$ with respect to each element of $W​$, and this is going to be the Jacobian that we talked about earlier, and if we look at this in this multiplication, $q = Wx​$, what's the derivative or the gradient of the first element of $q​$, so our first element up top, with respect to $W{}{_{11}}​$? So $q{}{_{1}}​$ with respect to $W{}{_{11}}​$? $x{}{_{1}}​$, so we know that this is $x{}{_{1}}​$, and we can write this out more generally of the gradient of $q{}{_{k}}​$ with respect to $W{}{_{ij}}​$, which is $x{}{_{j}}​$. And then now if we want to find the gradient of f with respect to each $W{}{_{ij}}​$. So looking at these derivatives now, we can use this chain rule that we talked earlier where we basically compound $df​$ over $dq{}{_k}​$ for each element of q with $dq{}{_k}​$ over $W{}{_{ij}}​$, for each element of $W​$. So we find the effect of each element of $W​$ on each element of $q​$, and sum this across all $q​$. And if you write this out, this is going to give this expression of $2q{}{_i}x{}{_j}​$. And so filling this out then we get this gradient with respect to $W​$, and so again we can compute this each element-wise, or we can also look at this expression that we've derived and write it out in vectorized form. Remember the important thing is always to check the gradient with respect to a variable should have the same shapes as the variable, and this is  something really useful in practice to sanity check, like once you've computed what your gradient should be, check that this is the same shape as your variable, because again, each element of your gradient is quantifying how much that element is contributing to your final output.

公式两边是指示函数，所以如果 $k=i$ 它们就是 1。

The both sides one is an indicator function, so this is saying that it's just one if $k = i$.

------
<center>
    <image src = "./images/4.1_18_vectorized-example3.png"></image>
</center>
------

最后任务是找到关于 $q{}{_i}​$ 的梯度。如果计算偏导数，$dq{}{_k}/dx{}{_i} = W{}{_{k,i}}​$，用计算 $W​$ 同样的方法计算，还是利用链式法则，得到全部的表达式。这就得到了关于 $x​$ 的梯度，它和 $x​$ 的大小一致，把它写成向量形式。

Now our last thing we need to find is the gradient with respect to $q{}{_i}$. So here if we compute the partial derivatives we can see that $dq{}{_k}/dx{}{_i} = W{}{_{k,i}}$, using the same way as we did it for W, and then again we can just use the chain rule and get the total expression for that. And so this is going to be the gradient with respect to $x$, again, of the same shape as $x$, and we can also write this out in vectorized form if we want.

## 4.1.4 模块化实现 Modularized Implementation

------
<center>
    <image src = "./images/4.1_19_forward-backwards-API1.png"></image>
</center>
<center>
    <image src = "./images/4.1_20_forward-backwards-API2.png"></image>
</center>
------

在这种情况下非常像模块化计算，在计算图中，观察每个本地节点计算本地梯度，然后利用逆向梯度链式传播下去。你可以把它看成一个前向和反向传播的 API，在前向传播中计算节点输出，反向传播中计算梯度，在实际运行的时候，我们也是按照这个方法做的。我们可以把它想象成门，当我们运用正向和反向传播函数，用链式法则计算反向传播的时候，如果我们有整个计算图，我们可以迭代得到所有图的正向传播，图中所有的节点，所有的门。这里门和节点是一回事，我们在所有的门上迭代，在每个门上前向传播。我们按照顺序的拓扑结构，运算所有到某个节点的输入。接着，反向传播按照相反的方向通过所有的门，得到每个门的反向传播。

So the way we've been thinking about this is like a really modularized implementation, where in our computational graph, we look at each node locally and we compute the local gradients and chain them with  upstream gradients coming down, and so you can think of this as basically a forward and a backwards API. In forward pass we implement the function computing the output of this node, and in the backwards pass we compute the gradient. And so when we actually implement this in code, we're going to do this in exactly the same way. We can think about, for each gate, if we implement a forward function and a backward function, where the backward function is computing the chain rule, then if we have our entire graph, we can just make a forward pass through the entire graph by iterating through all the nodes in the graph, all the gates. Here I'm using the word gate and node, we can iterate through all of these gates and just call forward on each of the gates. And we just want to do this in topologically sorted order, so we process all of the inputs coming in to a node before we process that node. And then going backwards, we're just going to go through all of the gates in this reverse sorted order, and then call backwards on each of these gates.

------
<center>
    <image src = "./images/4.1_21_forward-backwards-API3.png"></image>
</center>
------

如果我们看一下我们偏导数门的运行，在这个例子中，是一个乘法门，我们进行前向传播，输入 $x$ $y$ 输出 $z$。然后反向传播，输入 $dz$ 逆向梯度，希望输出的是对输入 $x$ $y$ 的梯度并传导下去。输出 $dx$ $dy$，在这个例子中，所有数值都是标量，如果我们看下前向传播，我们要注意有一个很重要的事情，我们需要缓存前向传播的数值，因为我们需要用它去计算反向传播很多次。所以在正向计算中，我们希望存储 $x$ 和 $y$ 这些值，在利用链式法则的反向传播中，将上游梯度的值，使用其他分支的值对它进行量化，并保持，计算 $dx$，我们需要使用已保存的 self, $y$，然后将它与 $dz$ 相乘，要计算 $dy$ 也是一样。

And if we look at the implementation for our particular gates, for example, this MultiplyGate here, we want to implement the forward pass, it gets $x$ and $y$ as inputs, and returns the value of $z$, and then we'll go backwards, we get as input $dz$， which is our upstream gradient, and we want to output the gradients on the inputs x and y to pass down, so we're going to output $dx$ and $dy$, so in this case, everything is back to the scalar case here, and if we look at this in the forward pass, one thing that's important is that we should cache the value of the forward pass, because we end up using this in the backward pass a lot of the time. So here in the forward pass, we want to cache the values of $x$ and $y$, and in the backward pass, using the chain rule, we're going to take the value of the upstream gradient and scale it by the value of the other branch, and so we'll keep, for $dx$ we'll take our value of self y that we kept, and multiply it by $dz$ coming down, and same for $dy$.

当你在看很多深度学习框架和库时，你就会发现他们恰恰都使用了这样的模块化设计。比如， Caffe 是一个流行的深度学习框架，如果你看了 Caffe 的代码，你可以看到一些目录，层和层之间，都是基本的计算节点，通常来说，层可能会更多一些，其中的一些是更复杂的计算节点，像之前我们提到的 sigmoid，你可以看到，基本上就是各种不同计算节点的整个列表。你可能会用到 sigmoid，卷积，Argmax，你会用到这些层。如果你深入去了解他们每一个，它们都会用到正向计算和反向计算，在我们构建的整个网络中，在进行正向反向计算时，所有这些都会被调用。所以我们的网络从根本上来说是由这些堆叠起来，就是由网络中我们所选择的不同的层来构成。

If you look at a lot of deep-learning frameworks and libraries you'll see that they exactly follow this kind of modularization. For example, Caffe is a popular deep learning framework, and if you go through the Caffe source code, you'll get to some directory that says layers and in layers, which are basically computational nodes, usually layers might be slightly more, some of these more complex computational nodes, like the sigmoid that we talked about earlier, you'll see, basically just a whole list of all different kinds of computational nodes. So you might have the sigmoid, or convolution, Argmax. And if you dig into each of them, they're just exactly implementing a forward pass and a backward pass, and all of these are called when we do forward and backward pass through the entire network that we formed, and so our network is just basically going to be stacking up all of these, the different layers that we choose to use in the network.

------
<center>
    <image src = "./images/4.1_22_Caffe-sigmoid-layer.png"></image>
</center>
------

我们来看一个具体的例子，sigmoid，你可以看到这里是正向计算，它只是根据 sigmoid 的表达式进行计算，然后是一个反向计算。将 top_diff 作为输入，在这个例子里它是上游梯度，将它与我们计算的本地梯度相乘，

So for example, if we look at a specific one, a sigmoid layer, you'll see that there's a forward pass which computes exactly the sigmoid expression, and then a backward pass, where it is taking as input something, basically a top_diff, which is our upstream gradient in this case, and multiplying it by a local gradient that we compute. 

## 4.1.5 总结 Summary

------
<center>
    <image src = "./images/4.1_23_summary.png"></image>
</center>
------

把刚才讲的总结一下。当你使用神经网络时，它们都会非常庞大和复杂，将所有参数的梯度公式写出来是不现实的。为了得到这些梯度，我们讲到了应该如何使用反向传播来计算梯度，而且这算是神经网络中的一个核心技术。在我们有了计算图标之后，这是一个链式法则的递归应用，我们从后开始进行反向计算出相对于所有中间变量的梯度，这些中间变量是你的输入、参数和中间所有的值。我们也谈到了这些计算和图表结构，你可以将它看做正向和反向计算的 API。所以在正向推导时，我们希望得到计算结果，并存储所有将会在后面的图计算中用到的中间值，在反向计算中，我们使用链式法则，用上游梯度，将它与本地梯度相乘计算在输出节点方向上的梯度，然后将它传递给下一个连接的节点。

So summary of what we've talked about so far. When we get down to work with neural networks, these are going to be really large and complex, so it's going to be impractical to write down the gradient formula by hand for all your parameters. So in order to get these gradients, we talked about how, what we should use is backpropagation, and this is kind of one of the core techniques of neural networks, is basically using backpropagation to get your gradients. And this is recursive application of the chain rule where we have this computational graph, and we start at the back and we go backwards through it to compute the gradients with respect to all of the intermediate variables, which are your inputs, your parameters, and everything else in the middle. And we've also talked about how really this implementation and this graph structure, each of these nodes is really, you can see this as implementing a forward and backwards API. And so in the forward pass we want to compute the results of the operation, and we want to save any intermediate values that we might want to use later in our gradient computation, and then in the backwards pass we apply this chain rule, and we take this upstream gradient, we chain it, multiply it with local gradient to compute the gradient with respect to the inputs of the node, and we pass this down to the nodes that are connected next.

